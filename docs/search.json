[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Environmental Data Science",
    "section": "",
    "text": "1 Welcome!\nThis site hosts the course curriculum for Colorado State University’s ESS 523A course: Introduction to Environmental Data Science"
  },
  {
    "objectID": "index.html#setting-up-r",
    "href": "index.html#setting-up-r",
    "title": "Introduction to Environmental Data Science",
    "section": "Setting up R",
    "text": "Setting up R\nAs your first step, please follow the instructions on the Setup Instructions page to make sure you have all the necessary software installed on your computer before the class starts."
  },
  {
    "objectID": "index.html#navigating-this-site",
    "href": "index.html#navigating-this-site",
    "title": "Introduction to Environmental Data Science",
    "section": "Navigating this site",
    "text": "Navigating this site\nThe table of contents on the left allows you to navigate to the lesson for each week of the course. Each lesson will walk you through the topic(s), analysis, etc. for that week. There are exercises at the end of the lesson that will be that week’s homework assignment.\nHomework will be submitted through Canvas and exercise questions (including any code, figures, etc.) must be submitted as a rendered R Markdown document in Word or HTML format. The Intro to R and R Markdown lesson will walk through how to create these R Markdown documents; you will use these R Markdowns as the basis for writing your code/answers to the exercises and then render them to either a Word or HTML report which is what you will submit through Canvas."
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Introduction to Environmental Data Science",
    "section": "Goals",
    "text": "Goals\nThe broad goal of this course is to learn the data science tools and best practices for working with environmental datasets using the programming language R. Specific to the content in this website, students will learn how to:\n\nNavigate the RStudio interface and create R Markdown documents for reproducible reporting;\nUtilize R packages and functions to manipulate and analyze data effectively and apply data wrangling techniques using the {tidyverse} framework;\nDifferentiate between various data types and structures within R; and\nExplore comparative analyses, linear regression, and trend analysis techniques to reveal patterns in data."
  },
  {
    "objectID": "index.html#approach-and-expectations",
    "href": "index.html#approach-and-expectations",
    "title": "Introduction to Environmental Data Science",
    "section": "Approach and Expectations",
    "text": "Approach and Expectations\nIn-Person Section:\nThis class is flipped, meaning all materials for the week ahead must be reviewed before class. To encourage this we will have weekly quizzes on pre-class content each Monday before we dive into the assignment.\nSo without lectures in class what do we do together? We code! This class has almost 6 hours of contact time per week, and we design lessons so that you should be able to finish your assignments in class. The flipped class allows for deeper discussion about the common pitfalls of coding and allows for collaborative work time with your fellow classmates.\nGenerally we will do a quick live-code review of concepts from the assignment and the pre-class materials, but more than 1 hour per class will be dedicated to you coding and working on the assignment in class.\nAs such, coming to class is a vital part of how you can be successful and we fully expect you to be there every day.\nWe also will actively encourage a collaborative coding environment where students help each other and discuss the best approach to solving various coding problems. We also hope that outside of class, you will use the course Teams channel to discuss code issues!\nOnline Sections:\nThe online course is asynchronous, and you will watch the class lectures that are posted on Canvas and work through lessons and assignments on your own time. While you will be learning in an online environment, we highly encourage you to collaborate with your fellow online classmates by communicating through the course Teams channel.\nAll Students:\nWe will send all course communications as Canvas announcements. The course syllabus will also be posted on Canvas. Please review the course syllabus for communication guidelines, AI policy, and other important course logistics and details."
  },
  {
    "objectID": "index.html#additional-introductory-resources",
    "href": "index.html#additional-introductory-resources",
    "title": "Introduction to Environmental Data Science",
    "section": "Additional introductory resources",
    "text": "Additional introductory resources\nIf you are looking to learn even more outside of what this class offers, below are some great introductory R resources:\n\nStat 158 - Vectors, data frames, installing R, etc…\nRStudio Materials - A series of videos, books, and more to get you started in R.\n\n\n1.0.0.1 Tidyverse Introduction\n\nR for Data Science - Covers all of the basic intro material, from a tidyverse perspective. As discussed, this is one way to find solutions in R, it happens to be my preferred way, but there are lots of Base R ways that work just fine!\nStat 159 - A CSU specific course for an intro to the tidyverse\nR Markdown - The primary book for learning more about R Markdown and all of its quirks.\nCheatsheets - Short, clear documents that cover so much material from dplyr to shiny apps. Great for quick references. We find the rmarkdown and ggplot2 ones particularly useful!"
  },
  {
    "objectID": "setup-instructions.html#install-r-and-rstudio",
    "href": "setup-instructions.html#install-r-and-rstudio",
    "title": "\n2  Setup Instructions\n",
    "section": "\n2.1 Install R and RStudio",
    "text": "2.1 Install R and RStudio\nR is an open source language and software environment for statistical analysis and graphics (plus so much more!). You must first download the R software before downloading R Studio (both free).\nInstall software here: https://posit.co/download/rstudio-desktop/\n\nFirst Install R. Follow the link and choose the correct download based on your operating system (Mac, Windows, Linux)\nThen, after installing R, click the second link to install RStudio.\n\nNote: If you already have R installed, you must have at least version 4.3.0. or greater, but it is best to have the most recent version installed (4.5.1)\nRStudio is an R Integrated Development Environment (IDE) that provides a built-in editor and other advantages such as version control and project management. This is the program we will be working in for the entire course."
  },
  {
    "objectID": "setup-instructions.html#package-installation",
    "href": "setup-instructions.html#package-installation",
    "title": "\n2  Setup Instructions\n",
    "section": "\n2.2 Package Installation",
    "text": "2.2 Package Installation\nWhile the R software comes with many pre-loaded functions (referred to as ‘base R’ functions) to perform various operations in R, there are thousands of R packages that provide additional reusable R functions. In order to use these functions you need to first install the package to your local machine using the install.packages() function. Once a package is installed on your computer you don’t need to install it again (but you may have to update it). Anytime you want to use the package in a new R session you can load it with the library() function.\nWe will be working in RStudio for this entire course, so after you have installed both R and RStudio, open a new session of RStudio.\nYou will learn more about the ins and outs of RStudio in class, but for set up purposes you will just be running code in the Console. Normally you want to save the code you write, but since package installation is only needed once (unless you are working on a new machine or need to update any packages) you can execute this directly in the console.\nRun the following three lines of code (one at a time) in the console. You can click the copy button in the upper right corner when you hover over the code chunk, then paste that after the > in the Console. Spelling and capitalization is important!\nInstall the {tidyverse} package. The Tidyverse is actually a collection of multiple R packages designed for data manipulation, exploration, and visualization that you are likely to use in every day data analysis. When you install the Tidyverse, it installs all of these packages, and you can later load all of them in your R session with library(tidyverse). Since you are installing multiple packages here this may take a little while.\n\ninstall.packages(\"tidyverse\")\n\nInstall the {palmerpenguins} package. This is a data package that installs a couple of spreadsheets you can load and work with in R.\n\ninstall.packages(\"palmerpenguins\")\n\nInstall the {rmarkdown} package. Later on in the course you will be working in and rendering R Markdown files and reports. R Markdown is a notebook style interface integrating text and code, allowing you to create fully reproducible documents and render them to various elegantly formatted static or dynamic outputs.\nYou can learn more about R Markdown at their website, which has really informative lessons on their Getting Started page, and see the range of outputs you can create at their Gallery page.\n\ninstall.packages(\"rmarkdown\")\n\nTo see if you successfully installed all three packages, use the library() function to load the packages into your session. You should either see nothing printed to the console after running library(), or in the case of the tidyverse you may see some messages printed. As long as there are no error messages, you should be all set! Please use our class’s Teams Channel for assistance if you get any error messages.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(rmarkdown)"
  },
  {
    "objectID": "introduction-to-r-rstudio-and-r-markdown.html#getting-to-know-rstudio",
    "href": "introduction-to-r-rstudio-and-r-markdown.html#getting-to-know-rstudio",
    "title": "\n3  Introduction to R, RStudio and R Markdown\n",
    "section": "\n3.1 Getting to know RStudio",
    "text": "3.1 Getting to know RStudio\nWhen you first open RStudio, it is split into 3 panels:\n\n\nThe Console (left), where you can directly type and run code (by hitting Enter)\n\nThe Environment/History pane (upper-right), where you can view the objects you currently have stored in your environment and a history of the code you’ve run\n\nThe Files/Plots/Packages/Help pane (lower-right), where you can search for files, view and save your plots, view and manage what packages are loaded in your library and session, and get R help.\n\n\n\nImage Credit: Software Carpentry\n\n\n\nTo write and save code you use .R scripts (or RMarkdown, which we will learn shortly). You can open a new script with File -> New File or by clicking the icon with the green plus sign in the upper left corner. When you open a script, RStudio then opens a fourth ‘Source’ panel in the upper-left to write and save your code. You can also send code from a script directly to the console to execute it by highlighting the entire code line/chunk (or place your cursor at the end of the code chunk) and hit CTRL+ENTER on a PC or CMD+ENTER on a Mac.\n\n\nImage Credit: Software Carpentry\n\n\nIt is good practice to add comments/notes throughout your scripts to document what the code is doing. To do this start a line with a #. R knows to ignore everything after a #, so you can write whatever you want there. Note that R reads line by line, so if you want your comments to carry over multiple lines you need a # at every line."
  },
  {
    "objectID": "introduction-to-r-rstudio-and-r-markdown.html#r-projects",
    "href": "introduction-to-r-rstudio-and-r-markdown.html#r-projects",
    "title": "\n3  Introduction to R, RStudio and R Markdown\n",
    "section": "\n3.2 R Projects",
    "text": "3.2 R Projects\nAs a first step whenever you start a new project, workflow, analysis, etc., it is good practice to set up an R project. R Projects are RStudio’s way of bundling together all your files for a specific project, such as data, scripts, results, figures. Your project directory also becomes your working directory, so everything is self-contained and easily portable.\nWe recommend using a single R Project (i.e., contained in a single folder) for this course, so lets create one now.\nYou can start an R project in an existing directory or in a new one. To create a project go to File -> New Project:\n\nLet’s choose ‘New Directory’ then ‘New Project’. Now choose a directory name, this will be both the folder name and the project name, so use proper spelling conventions (no spaces!). We recommend naming it something course specific, like ‘WR-696-2023’, or even more generic ‘Intro-R-Fall23’. Choose where on your local file system you want to save this new folder/project (somewhere you can find it easily), then click ‘Create Project’.\nNow you can see your RStudio session is working in the R project you just created. You can see the working directory printed at the top of your console is now the project directory, and in the ‘Files’ tab in RStudio you can see there is an .Rproj file with the same name as the R project, which will open up this R project in RStudio whenever you come back to it.\nTest out how this .Rproj file works. Close out of your R session, navigate to the project folder on your computer, and double-click the .Rproj file.\n\nWhat is a working directory? A working directory is the default file path to a specific file location on your computer to read files from or save files to. Since everyone’s computer is unique, everyone’s full file paths will be different. This is an advantage of working in R Projects, you can use relative file paths, since the working directory defaults to wherever the .RProj file is saved on your computer you don’t need to specify the full unique path to read and write files within the project directory."
  },
  {
    "objectID": "introduction-to-r-rstudio-and-r-markdown.html#write-a-set-up-script",
    "href": "introduction-to-r-rstudio-and-r-markdown.html#write-a-set-up-script",
    "title": "\n3  Introduction to R, RStudio and R Markdown\n",
    "section": "\n3.3 Write a set-up script",
    "text": "3.3 Write a set-up script\nLet’s start coding!\nThe first thing you do in a fresh R session is set up your environment, which mostly includes installing and loading necessary libraries and reading in required data sets. Let’s open a fresh R script and save it in our root (project) directory. Call this script ‘setup.R’.\n\n3.3.1 Functions\nBefore creating a set up script, it might be helpful to understand the use of functions in R if you are new to this programming language. R has many built-in functions to perform various tasks. To run these functions you type the function name followed by parentheses. Within the parentheses you put in your specific arguments needed to run the function.\n\n# mathematical functions with numbers\nlog(10)\n\n[1] 2.302585\n\n# average a range of numbers\nmean(1:5)\n\n[1] 3\n\n# nested functions for a string of numbers, using the concatenate function 'c'\nmean(c(1,2,3,4,5))\n\n[1] 3\n\n# functions with characters\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\npaste(\"Hello\", \"World\", sep = \"-\")\n\n[1] \"Hello-World\"\n\n\n\n3.3.2 Packages\nR Packages include reusable functions that are not built-in with R. To use these functions, you must install the package to your local system with the install.packages() function. Once a package is installed on your computer you don’t need to install it again (you will likely need to update it at some point though). Anytime you want to use the package in a new R session you load it with the library() function.\nWhen do I use :: ?\nIf you have a package installed, you don’t necessarily have to load it in with library() to use it in your R session. Instead, you can type the package name followed by :: and use any functions in that package. This may be useful for some one-off functions using a specific package, however if you will be using packages a lot throughout your workflow you will want to load it in to your session. You should also use :: in cases where you have multiple packages loaded that may have conflicting functions (e.g., plot() in Base R vs. plot() in the {terra} package).\n\n3.3.2.1 Base R vs. The Tidyverse\nYou may hear us use the terms ‘Base R’ and ‘Tidyverse’ a lot throughout this course. Base R includes functions that are installed with the R software and do not require the installation of additional packages to use them. The Tidyverse is a collection of R packages designed for data manipulation, exploration, and visualization that you are likely to use in every day data analysis, and all use the same design philosophy, grammar, and data structures. When you install the Tidyverse, it installs all of these packages, and you can then load all of them in your R session with library(tidyverse). Base R and the Tidyverse have many similar functions, but many prefer the style, efficiency and functionality of the Tidyverse packages, and we will mostly be sticking to Tidyverse functions for this course.\n\n3.3.2.2 Package load function\nTo make code reproducible (meaning anyone can run your code from their local machines) we can write a function that checks whether or not necessary packages are installed, if not install them and load them, or if they are already installed it will only load them and not re-install. This function looks like:\n\npackageLoad <-\n  function(x) {\n    for (i in 1:length(x)) {\n      if (!x[i] %in% installed.packages()) {\n        install.packages(x[i])\n      }\n      library(x[i], character.only = TRUE)\n    }\n  }\n\nFor each package name given (‘x’) it checks if it is already installed, if not installs it, and then loads that package into the session. In future lessons we will learn more about writing custom functions, and iterating with for loops, but for now you can copy/paste this function and put it at the top of your set up script. When you execute this chunk of code, you won’t see anything printed in the console, however you should now see packageLoad() in your Environment under ‘Functions’. You can now use this function as many times as you want. Test is out, and use it to install the Tidyverse package(s).\n\npackageLoad('tidyverse')\n\nYou can also give this function a string of package names. Lets install all the packages we will need for the first week, or if you already followed the set up instructions, this will just load the packages into your session since you already installed them.\n\n# create a string of package names\npackages <- c('tidyverse',\n              'palmerpenguins',\n              'rmarkdown')\n# use the packageLoad function we created on those packages\npackageLoad(packages)\n\nSince this is code you will be re-using throughout your workflows, we will save it as its own script and run it at the beginning of other scripts/documents using the source() function as a part of our reproducible workflows."
  },
  {
    "objectID": "introduction-to-r-rstudio-and-r-markdown.html#r-markdown",
    "href": "introduction-to-r-rstudio-and-r-markdown.html#r-markdown",
    "title": "\n3  Introduction to R, RStudio and R Markdown\n",
    "section": "\n3.4 R Markdown",
    "text": "3.4 R Markdown\nThroughout this course you will be working mostly in R Markdown documents. R Markdown is a notebook style interface integrating text and code, allowing you to create fully reproducible documents and render them to various elegantly formatted static or dynamic outputs (which is how you will be submitting your assignments).\nYou can learn more at the R Markdown website, which has really informative lessons on the Getting Started page and you can see the range of outputs you can create at the Gallery page.\n\n3.4.1 What About Quarto?\nSome of you may have heard of Quarto, which is essentially an extension of R Markdown but it lives as its own software to allow its use in other languages such as Python, Julia and Observable. You can install the Quarto CLI on its own and RStudio will detect it so you can create documents within the IDE, or alternatively with newer versions of RStudio a version of Quarto is built-in and you can enable Quarto through the R Markdown tab in Global Options. R Markdown isn’t going anywhere, however many in the data science realm are switching to Quarto. Quarto documents are very similar to R Markdown, in fact Quarto can even render R Markdown documents, so after learning R Markdown in this course you should have some of the fundamental skills to easily switch to Quarto if you want to. You can read more about Quarto here.\n\n3.4.2 Getting started with R Markdown\nLet’s create a new document by going to File -> New File -> R Markdown. You will be prompted to add information like title and author, fill those in (let’s call it “Intro to R and R Markdown”) and keep the output as HTML for now. Click OK to create the document.\nThis creates an outline of an R Markdown document, and you see the title, author and date you gave the prompt at the top of the document which is called the YAML header.\nNotice that the file contains three types of content:\n\nAn (optional) YAML header surrounded by ---s\nR code chunks surrounded by ```s\ntext mixed with simple text formatting\n\nSince this is a notebook style document, you run the code chunks by clicking the green play button in the top right corner of each code chunk, and then the output is returned directly below the chunk.\n\nIf you’d rather have the code chunk output go to the console instead of directly below the chunk in your R Markdown document, go to Tools -> Global Options -> R Markdown and uncheck “Show output inline for all R Markdown documents”\n\nWhen you want to create a report from your notebook, you render it by hitting the ‘knit’ button at the top of the Source pane (with the ball of yarn next to it), and it will render to the format you have specified in the YAML header. In order to do so though, you need to have the {rmarkdown} package installed.\nYou can delete the rest of the code/text below the YAML header, and insert a new code chunk at the top. You can insert code chunks by clicking the green C with the ‘+’ sign at the top of the source editor, or with the keyboard short cut (Ctrl+Alt+I for Windows, Option+Command+I for Macs). For the rest of the lesson (and course) you will be writing and executing code through code chunks, and you can type any notes in the main body of the document.\nThe first chunk is almost always your set up code, where you read in libraries and any necessary data sets. Here we will execute our set up script to install and load all the libraries we need:\n\nsource(\"setup.R\")"
  },
  {
    "objectID": "introduction-to-r-rstudio-and-r-markdown.html#explore",
    "href": "introduction-to-r-rstudio-and-r-markdown.html#explore",
    "title": "\n3  Introduction to R, RStudio and R Markdown\n",
    "section": "\n3.5 Explore",
    "text": "3.5 Explore\nNormally when working with a new data set, the first thing we do is explore the data to better understand what we’re working with. To do so, you also need to understand the fundamental data types and structures you can work with in R.\n\n3.5.1 The penguins data\nFor this intro lesson, we are going to use the Palmer Penguins data set (which is loaded with the {palmerpenguins} package you installed in your set up script). This data was collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\nLoad the penguins data set.\n\ndata(\"penguins\")\n\nYou now see it in the Environment pane. Print it to the console to see a snapshot of the data:\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>\n\n\n\n3.5.2 Data Types\nThis data is structured as a data frame, probably the most common data type and one you are most familiar with. These are like Excel spreadsheets: tabular data organized by rows and columns. However we see at the top this is called a tibble which is just a fancy kind of data frame specific to the Tidyverse.\nAt the top we can see the data type of each column. There are five main data types:\n\ncharacter: \"a\", \"swc\"\nnumeric: 2, 15.5\ninteger: 2L (the L tells R to store this as an integer)\nlogical: TRUE, FALSE\ncomplex: 1+4i (complex numbers with real and imaginary parts)\n\nData types are combined to form data structures. R’s basic data structures include:\n\natomic vector\nlist\nmatrix\ndata frame\nfactors\n\nYou can see the data type or structure of an object using the class() function, and get more specific details using the str() function. (Note that ‘tbl’ stands for tibble).\n\nclass(penguins)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\n\nclass(penguins$species)\n\n[1] \"factor\"\n\nstr(penguins$species)\n\n Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWhen we pull one column from a data frame like we just did above using the $ operator, that returns a vector. Vectors are 1-dimensional, and must contain data of a single data type (i.e., you cannot have a vector of both numbers and characters).\nIf you want a 1-dimensional object that holds mixed data types and structures, that would be a list. You can put together pretty much anything in a list.\n\nmyList <- list(\"apple\", 1993, FALSE, penguins)\nstr(myList)\n\nList of 4\n $ : chr \"apple\"\n $ : num 1993\n $ : logi FALSE\n $ : tibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n  ..$ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n  ..$ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n  ..$ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n  ..$ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n  ..$ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n  ..$ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n  ..$ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n  ..$ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nYou can even nest lists within lists:\n\nlist(myList, list(\"more stuff here\", list(\"and more\")))\n\n[[1]]\n[[1]][[1]]\n[1] \"apple\"\n\n[[1]][[2]]\n[1] 1993\n\n[[1]][[3]]\n[1] FALSE\n\n[[1]][[4]]\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>\n\n\n[[2]]\n[[2]][[1]]\n[1] \"more stuff here\"\n\n[[2]][[2]]\n[[2]][[2]][[1]]\n[1] \"and more\"\n\n\nYou can use the names() function to retrieve or assign names to list and vector elements:\n\nnames(myList) <- c(\"fruit\", \"year\", \"logic\", \"data\")\nnames(myList)\n\n[1] \"fruit\" \"year\"  \"logic\" \"data\" \n\n\n\n3.5.3 Indexing\nIndexing is an extremely important aspect to data exploration and manipulation. In fact you already started indexing when we looked at the data type of individual columns with penguins$species. How you index is dependent on the data structure.\nIndex lists:\n\n# for lists we use double brackes [[]]\nmyList[[1]] # select the first stored object in the list\n\n[1] \"apple\"\n\nmyList[[\"data\"]] # select the object in the list named \"data\" (a data frame)\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>\n\n\nIndex vectors:\n\n# for vectors we use single brackets []\nmyVector <- c(\"apple\", \"banana\", \"pear\")\nmyVector[2]\n\n[1] \"banana\"\n\n\nIndex data frames:\n\n# dataframe[row(s), columns()]\npenguins[1:5, 2]\n\npenguins[1:5, \"island\"]\n\npenguins[1, 1:5]\n\npenguins[1:5, c(\"species\",\"sex\")]\n\npenguins[penguins$sex=='female',]\n\n# $ for a single column\npenguins$species\n\n\nTo index elements of a list you must use double brackets [[ ]], and to index vectors and data frames you use single brackets [ ]\n\n\n3.5.3.1 Exercises\n(not required, but work through them if you want!)\n\n\nWhy don’t the following lines of code work? Tweak each one so the code runs\n\nmyList[\"Fruit\"]\n\n\npenguins$flipper_lenght_mm\n\n\npenguins[island=='Dream',]\n\n\nHow many species are in the penguins data set? What islands were the data collected for? (Note: the unique() function might help)\nUse indexing to create a new data frame that has only 3 columns: species, island and flipper length columns, and subset all rows for just the ‘Dream’ island.\nUse indexing and the mean() function to find the average flipper length for the Adelie species on Dream island. (Note: explore the mean() function and how to deal with NA values).\n\n\n3.5.4 The {dplyr} package\nSo far the code you’ve been writing has consisted of Base R functionality. Now lets dive into the Tidyverse with the {dplyr} package.\n{dplyr} is a Tidyverse package to handle most of your data exploration and manipulation tasks. Now that you have learned indexing, you may notice the first two {dplyr} functions you are going to learn. filter() and select() act as indexing functions by subsetting rows and columns based on specified names and/or conditions.\nSubset rows with filter()\nYou can filter data in many ways using logical operators (>, >=, <, <=, != (not equal), and == (equal)), AND (&), OR (|), and NOT (!) operators, and other operations such as %in%, which returns everything that matches at least one of the values in a given vector, and is.na() and !is.na() to return all missing or all non-missing data.\n\nfilter(penguins, species == \"Adelie\")\n\nfilter(penguins, species != \"Adelie\")\n\nfilter(penguins, island %in% c(\"Dream\", \"Torgersen\") & !is.na(bill_length_mm))\n\nNote: Tidyverse package functions take in column names without quotations.\n\nUsing {dplyr} functions will not manipulate the original data, so if you want to save the returned object you need to assign it to a new variable.\n\nSelect columns with select()\nselect() has many helper functions you can use with it, such as starts_with(), ends_with(), contains() and many more that are very useful when dealing with large data sets. See ?select for more details.\n\nWriting out ? ahead of any function from a package will open a description of that function in the “Help” pane.\n\n\n# Select two specific variables\nselect(penguins, species, sex)\n\n# Select a range of variables\nselect(penguins, species:flipper_length_mm)\n\n# Rename columns within select\nselect(penguins, genus = species, island)\n\n# Select column variables that are recorded in mm\nselect(penguins, contains(\"mm\"))\n\nCreate new variables with mutate()\n\n# New variable that calculates bill length in cm\nmutate(penguins, bill_length_cm = bill_length_mm/10)\n\n# mutate based on conditional statements\nmutate(penguins, species_sex = if_else(sex == 'male', paste0(species,\"_m\"), paste0(species, \"_f\")))\n\nNotice the use of paste0() here, and when we briefly used a similar function paste() in the ‘Functions’ section above. Explore the difference between these two. They are both very useful functions for pasting strings together.\ngroup_by() and summarise()\nThese can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire data set to operating on it group-by-group. group_by() becomes even more powerful when used along with summarise() to calculate some specified summary statistic for each group. However before we start using multiple operations in conjunction with one another, we need to talk about the pipe operator %>%.\n\n3.5.4.1 The pipe %>%\n\nThe pipe, %>%, comes from the magrittr package by Stefan Milton Bache. Packages in the Tidyverse load %>% for you automatically, so you don’t usually load {magrittr} explicitly. Pipes are a powerful tool for clearly expressing a sequence of multiple operations.\nFor example, the pipe operator can take this sequence of operations:\n\ndf1 <- filter(penguins, island == \"Dream\")\ndf2 <- mutate(df1, flipper_length_cm = flipper_length_mm/10)\ndf3 <- select(df2, species, year, flipper_length_cm)\n\nprint(df3)\n\nAnd turn it into this, removing the need to create intermediate variables\n\npenguins %>% \n  filter(island == \"Dream\") %>% \n  mutate(flipper_length_cm = flipper_length_mm/10) %>% \n  select(species, year, flipper_length_cm)\n\nYou can read it as a series of imperative statements: filter, then mutate, then select. A good way to pronounce %>% when reading code is “and then”. It takes the output of the operation to the left of %>% and feeds it into the next function as the input.\nSay you want to summarize data by some specified group, for example you want to find the average body mass for each species, this is where the group_by() function comes into play.\n\npenguins %>% \n  group_by(species) %>% \n  summarise(body_mass_avg = mean(body_mass_g, na.rm = TRUE))\n\nOr get a count of how many individuals were observed for each species each year\n\npenguins %>% \n  group_by(species, year) %>% \n  summarise(n_observations = n())\n\nYou can even shorten the above operation by using count() instead of summarise.\n\n3.5.4.2 Exercises\n(not required, but useful if you want to work through them!)\n\nReorder the variables in penguins so that year is the first column followed by the rest (Hint: look into the use of everything()).\nCreate a new column called ‘size_group’ where individuals with body mass greater than the overall average are called ‘large’ and those smaller are called ‘small’.\nFind out which year for each species had the largest average body mass.\nYou want to filter data for years that are not in a vector of given years, but this code doesn’t work. Tweak it so that it does. (Yes, you could just filter year to equal 2007 in this case but there is a trouble-shooting lessons here).\n\n\n    penguins %>% \n      filter(year !%in% c(2008, 2009))"
  },
  {
    "objectID": "introduction-to-r-rstudio-and-r-markdown.html#read-and-write-data",
    "href": "introduction-to-r-rstudio-and-r-markdown.html#read-and-write-data",
    "title": "\n3  Introduction to R, RStudio and R Markdown\n",
    "section": "\n3.6 Read and Write Data",
    "text": "3.6 Read and Write Data\nWe used an R data package today to read in our data frame, but that probably isn’t how you will normally read in your data.\nThere are many ways to read and write data in R. To read in .csv files, you can use read_csv() which is included in the Tidyverse with the {readr} package, and to save csv files use write_csv(). The {readxl} package is great for reading in excel files, however it is not included in the Tidyverse and will need to be loaded separately."
  },
  {
    "objectID": "exploratory-data-analysis.html#data-wrangling",
    "href": "exploratory-data-analysis.html#data-wrangling",
    "title": "\n4  Exploratory Data Analysis\n",
    "section": "\n4.1 Data wrangling",
    "text": "4.1 Data wrangling\n\n4.1.1 The dplyr package\ndplyr is a Tidyverse package to handle most of your data exploration and manipulation tasks. Now that you have learned indexing in the [Intro to R lesson][Introduction to R and RStudio], you may notice the first two dplyr functions we are going to learn, filter() and select() act as indexing functions, subsetting rows and columns based on specified names and/or conditions.\nSubset rows with filter()\nYou can filter data in many ways using logical operators (>, >=, <, <=, != (not equal), and == (equal)), AND (&), OR (|), and NOT (!) operators, and other operations such as %in%, which returns everything that matches at least one of the values in a given vector, and is.na() and !is.na() to return all missing or all non-missing data.\n\n# filter rows for just the Adelie species\nfilter(penguins, species == \"Adelie\")\n\n# filter rows for all species EXCEPT Adelie\nfilter(penguins, species != \"Adelie\")\n\n# filter islands Dream and Torgersen AND rows that DO NOT have  missing values for bill length\nfilter(penguins, island %in% c(\"Dream\", \"Torgersen\") & !is.na(bill_length_mm))\n\n\nNote: Tidyverse package functions take in column names without quotations.\n\nUsing dplyr functions will not manipulate the original data, so if you want to save the returned object you need to assign it to a new variable.\n\nbody_mass_filtered <- filter(penguins, body_mass_g > 4750 | body_mass_g < 3550)\n\nSubset columns with select()\nselect() has many helper functions you can use with it, such as starts_with(), ends_with(), contains() and many more that are very useful when dealing with large data sets. See ?select for more details\n\n# Select two specific variables\nselect(penguins, species, sex)\n\n# Select a range of variables\nselect(penguins, species:flipper_length_mm)\n\n# Rename columns within select\nselect(penguins, genus = species, island)\n\n# Select column variables that have 'mm' in their name\nselect(penguins, contains(\"mm\"))\n\nCreate new variables with mutate()\nmutate() allows you to edit existing columns or create new columns in an existing data frame, and you can perform calculations on existing columns to return outputs in the new column. The syntax is the name of the new column you want to make (or the current column you want to edit) on the left of =, and then to the right is what you want to put in the new column. Note that mutate() works row wise on the data frame.\n\n# New variable that calculates bill length in cm\nmutate(penguins, bill_length_cm = bill_length_mm/10)\n\n# mutate based on conditional statements with if_else()\nmutate(penguins, species_sex = if_else(sex == 'male', paste0(species,\"_m\"), paste0(species, \"_f\")))\n\nif_else() reads as: IF the given argument (sex == 'male') is TRUE, put this: paste0(species,\"_m\") otherwise if FALSE put this: paste0(species, \"_f\")\n\nNotice the use of paste0() here, and when we briefly used a similar function paste() in the ‘Functions’ section of the Intro to R lesson. Explore the difference between these two. They are both very useful functions for creating new character strings.\n\n\n4.1.1.1 The pipe %>%\n\nThe pipe, %>%, comes from the magrittr package by Stefan Milton Bache. Packages in the tidyverse load %>% for you automatically, so you don’t usually load magrittr explicitly. Pipes are a powerful tool for clearly expressing a sequence of multiple operations.\nFor example, the pipe operator can take this sequence of operations:\n\ndf1 <- filter(penguins, island == \"Dream\")\ndf2 <- mutate(df1, flipper_length_cm = flipper_length_mm/10)\ndf3 <- select(df2, -flipper_length_mm) # keep everything BUT the flipper_length_mm column\n\nprint(df3)\n\nAnd turn it into this, removing the need to create intermediate variables\n\npenguins %>% \n  filter(island == \"Dream\") %>% \n  mutate(flipper_length_cm = flipper_length_mm/10) %>% \n  select(-flipper_length_cm)\n\nYou can read it as a series of imperative statements: filter, then mutate, then select. A good way to pronounce %>% when reading code is “then”. It takes the output of the operation to the left of %>% and feeds it into the next function as the input.\ngroup_by() and summarise()\nAll the above functions can all be used in conjunction with group_by(), which changes the scope of each function from operating on the entire data set to instead operate by specified groups. group_by() becomes even more powerful when used along with summarise() to calculate some specified summary statistic for each group.\nSay you want to summarize data by some specified group, for example you want to find the average body mass for each species, this is how you could do that:\n\npenguins %>% \n  group_by(species) %>% \n  summarise(body_mass_avg = mean(body_mass_g, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  species   body_mass_avg\n  <fct>             <dbl>\n1 Adelie            3701.\n2 Chinstrap         3733.\n3 Gentoo            5076.\n\n\n\nNotice the additional argument na.rm = TRUE within the mean() function. This is required for any mathamatical or statistical functions if you have ANY missing (NA) values in your dataset. What this does is remove those values from the calculation, otherwise mean() would just return NA without the na.rm = TRUE argument.\n\nThe output now only has 3 rows, one for each unique group (i.e., species), and a new column with the calculated average body mass for each species.\nYou can also group by multiple variables. Say you want to calculate the sample size (i.e., count, which can be calculated with the n() function) for each species for each year of the study:\n\npenguins %>% \n  group_by(species, year) %>% \n  summarise(n_observations = n())\n\n# A tibble: 9 × 3\n# Groups:   species [3]\n  species    year n_observations\n  <fct>     <int>          <int>\n1 Adelie     2007             50\n2 Adelie     2008             50\n3 Adelie     2009             52\n4 Chinstrap  2007             26\n5 Chinstrap  2008             18\n6 Chinstrap  2009             24\n7 Gentoo     2007             34\n8 Gentoo     2008             46\n9 Gentoo     2009             44\n\n\nYou can even shorten the above operation by using a new function, count(), instead of summarise():\n\npenguins %>% \n  group_by(species, year) %>% \n  count()"
  },
  {
    "objectID": "exploratory-data-analysis.html#visualization",
    "href": "exploratory-data-analysis.html#visualization",
    "title": "\n4  Exploratory Data Analysis\n",
    "section": "\n4.2 Visualization",
    "text": "4.2 Visualization\nAn important part of data exploration includes visualizing the data to reveal patterns you can’t necessarily see from viewing a data frame of numbers. Here we are going to walk through a very quick introduction to ggplot2, using some code examples from the palmerpenguins R package tutorial: https://allisonhorst.github.io/palmerpenguins/articles/intro.html.\nggplot2 is perhaps the most popular data visualization package in the R language, and is also a part of the Tidyverse. One big difference about ggplot though is that it does not use the pipe %>% operator like we just learned, but instead threads together arguments with + signs (but you can pipe a data frame into the first ggplot() argument).\nThe general structure for ggplots follows the template below. Note that you can also specify the aes() parameters within ggplot() instead of your geom function, which you may see a lot of people do. The mappings include arguments such as the x and y variables from your data you want to use for the plot. The geom function is the type of plot you want to make, such as geom_point(), geom_bar(), etc, there are a lot to choose from.\n\n# general structure of ggplot functions\nggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))\n\nVisualize variable distributions with geom_historgram()\nIf you plan on doing any statistical analysis on your data , one of the first things you are likely to do is explore the distribution of your variables. You can plot histograms with geom_histogram()\n\nggplot(penguins) + \n  geom_histogram(mapping = aes(x = flipper_length_mm))\n\n\n\n\nThis tells us there may be a lot of variation in flipper size among species. We can use the ‘fill =’ argument to color the bars by species, and scale_fill_manual() to specify the colors.\n\n# Histogram example: flipper length by species\nggplot(penguins) +\n  geom_histogram(aes(x = flipper_length_mm, fill = species), alpha = 0.5, position = \"identity\") +\n  scale_fill_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\"))\n\n\n\n\nCool, now we can see there seems to be some pretty clear variation in flipper size among species. Another way to visualize across groups is with facet_wrap(), which will create a separate plot for each group, in this case species.\n\nggplot(penguins) +\n  geom_histogram(aes(x = flipper_length_mm, fill = species), alpha = 0.5, position = \"identity\") +\n  scale_fill_manual(values = c(\"darkorange\",\"darkorchid\",\"cyan4\")) +\n  facet_wrap(~species)\n\n\n\n\nCompare sample sizes with geom_bar()\nLet’s use ggplot to see sample size for each species on each island.\n\nggplot(penguins) +\n  geom_bar(mapping = aes(x = island, fill = species))\n\n\n\n\nAs you may have already noticed, the beauty about ggplot2 is there are a million ways you can customize your plots. This example builds on our simple bar plot:\n\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(alpha = 0.8) +\n  scale_fill_manual(values = c(\"darkorange\",\"purple\",\"cyan4\"), \n                    guide = FALSE) +\n  theme_minimal() +\n  facet_wrap(~species, ncol = 1) +\n  coord_flip()\n\n\n\n\nThis is important information, since we know now that not all species were sampled on every island, which will have complications for any comparisons we may want to make among islands.\nVisualize variable relationships with geom_point()\nWe can use geom_point() to view the relationship between two continuous variables by specifying the x and y axes. Say we want to visualize the relationship between penguin body mass and flipper length and color the points by species:\n\nggplot(penguins) +\n  geom_point(mapping = aes(x = body_mass_g, y = flipper_length_mm, color = species))"
  },
  {
    "objectID": "exploratory-data-analysis.html#exercises",
    "href": "exploratory-data-analysis.html#exercises",
    "title": "\n4  Exploratory Data Analysis\n",
    "section": "\n4.3 Exercises",
    "text": "4.3 Exercises\nYou must include the line(s) of code used to answer each question\n\nWhy don’t the following lines of code work? Tweak each one so the code runs (3 pts.)\n\n\npenguins[1:5, (\"species\", \"island\")]\n\n\npenguins$flipper_lenght_mm\n\n\npenguins[island=='Dream',]\n\n\nFind the average flipper length for each species. Which species has the largest flippers? (2 pts.)\n\n\n\n\n\nWhich is the only species that was sampled across all three islands in this study? You must use {dplyr} functions to answer this question (e.g., group_by() …) (2 pts.)\n\n\n\n\n\nReorder the variables in penguins so that year is the first column followed by the rest (Hint: look into the use of everything()). (2 pts.)\n\n\n\n\n\nCreate a new column called ‘size_group’ where individuals with body mass greater than the overall average are called ‘large’ and those smaller are called ‘small’. (Note: this answer requires the additional use of both the if_else() and mean() functions. Remember how to deal with NA values in mean()). (4 pts.)\n\n\n\n\n\nYou want to filter data for years that are not in a vector of given years, but this code doesn’t work. Tweak it so that it does. (Yes, you could just filter year to equal 2007 in this case but there is a trouble-shooting lessons here). (2 pts.)\n\n\npenguins %>% \n  filter(year !%in% c(2008, 2009))\n\n\n\n\nPlease include the line(s) of code you used to create your figure and make sure the figure is shown in the rendered report.\n\nUsing the visualization techniques you learned today, create a figure that allows you to visualize some comparison of your choice among the penguins data set. Below your figure write a testable hypothesis about the data and the patterns you see from this figure. (5 pts.)"
  },
  {
    "objectID": "data-visualization-in-r.html#publication-ready-figures-with-ggplot2",
    "href": "data-visualization-in-r.html#publication-ready-figures-with-ggplot2",
    "title": "\n5  Data Visualization in R\n",
    "section": "\n5.1 Publication Ready Figures with ggplot2\n",
    "text": "5.1 Publication Ready Figures with ggplot2\n\nFor this exercise you will learn how to spruce up your ggplot2 figures with theme customization, annotation, color palettes, and more.\nTo demonstrate some of these advanced visualization techniques, we will be analyzing the relationships among some census data for Larimer county.\nLet’s start with this basic plot:\n\ncensus_data %>% \n  ggplot(aes(x = median_age, y = percent_bipoc))+\n  geom_point(color = \"black\")\n\n\n\n\nAnd by the end of this lesson turn it into this:\n\n\n5.1.1 General Appearance\n\n5.1.1.1 Customize points within geom_point()\n\n\ncolor or size points by a variable or apply a specific color/number\nchange the transparency with alpha (ranges from 0-1)\n\n\n#specific color and size value\ncensus_data %>% \n  ggplot(aes(x = median_age, y = percent_bipoc))+\n  geom_point(color = \"red\", size = 4, alpha = 0.5)\n\n\n\n\nWhen sizing or coloring points by a variable in the dataset, it goes within aes():\n\n# size by a variable\ncensus_data %>% \n  ggplot(aes(x = median_age, y = percent_bipoc))+\n  geom_point(aes(size = median_income), color = \"red\")\n\n\n\n\n\n# color by a variable\ncensus_data %>% \n  ggplot(aes(x = median_age, y = percent_bipoc))+\n  geom_point(aes(color = median_income), size = 4)\n\n\n\n\n\n5.1.1.2 Titles and limits\n\nadd title with ggtitle\nedit axis labels with xlab() and ylab()\nchange axis limits with xlim() and ylim()\n\n\ncensus_data %>% \n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\")+\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\")+\n  xlab(\"Median Age\")+\n  ylab(\"People of Color (%)\")+\n  xlim(c(20, 70))+\n  ylim(c(0, 35))\n\n\n\n\nBe cautious of setting the axis limits however, as you notice it omits the full dataset which could lead to dangerous misinterpretations of the data.\nYou can also put multiple label arguments within labs() like this:\n\ncensus_data %>% \n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\")+\n  labs(\n    title = \"Census Tract socioeconomic data for Larimer County\",\n    x = \"Median Age\",\n    y = \"People of Color (%)\"\n  ) +\n  xlim(c(20, 70))+\n  ylim(c(0, 35))\n\n\n\n\n\n5.1.1.3 Chart components with theme()\n\nAll ggplot2 components can be customized within the theme() function. The full list of editable components (there’s a lot!) can be found here. Note that the functions used within theme() depend on the type of components, such as element_text() for text, element_line() for lines, etc.\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\") +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\") +\n  theme(\n    #edit plot title\n    plot.title = element_text(size = 16, color = \"blue\"),\n    # edit x axis title\n    axis.title.x = element_text(face = \"italic\", color = \"orange\"),\n    # edit y axis ticks\n    axis.text.y = element_text(face = \"bold\"),\n    # edit grid lines\n    panel.grid.major = element_line(color = \"black\"),\n\n  )\n\n\n\n\nAnother change you may want to make is the value breaks in the axis labels (i.e., what values are shown on the axis). To customize that for a continuous variable you can use scale_x_continuous() / scale_y_continuous (for discrete variables use scale_x_discrete ). In this example we will also add angle = to our axis text to angle the labels so they are not too jumbled:\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\") +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\") +\n  scale_x_continuous(breaks = seq(15, 90, 5))+\n  theme(\n    # angle axis labels\n    axis.text.x = element_text(angle = 45)\n  )\n\n\n\n\nWhile these edits aren’t necessarily pretty, we are just demonstrating how you would edit specific components of your charts. To edit overall aesthetics of your plots you can change the theme.\n\n5.1.1.4 Themes\nggplot2 comes with many built in theme options (see the complete list here).\nFor example, see what theme_minimal() and theme_classic() look like:\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\") +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  theme_minimal()\n\n\n\n\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\") +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  theme_classic()\n\n\n\n\nYou can also import many different themes by installing certain packages. A popular one is ggthemes. A complete list of themes with this package can be seen here\nIf you did not add this to your setup.R script yet, to run this example, first install the ggthemes package and then load it in to your session:\nNote: you should NOT include any install.packages() lines of code in your .Rmd when you try to knit, it will likely throw an error. Remember you only need to use install.packages() once.\n\ninstall.packages(\"ggthemes\")\n\n\nlibrary(ggthemes)\n\nNow explore a few themes, such as theme_wsj, which uses the Wall Street Journal theme, and theme_economist and theme_economist_white to use themes used by the Economist.\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\") +\n  ggtitle(\"Socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  ggthemes::theme_wsj()+\n  # make the text smaller\n  theme(text = element_text(size = 8))\n\n\n\n\n\nNote you may need to click ‘Zoom’ in the Plot window to view the figure better.\n\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\") +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  ggthemes::theme_economist()\n\n\n\n\nSome themes may look messy out of the box, but you can apply any elements from theme() afterwards to clean it up. For example, change the legend position:\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income), color = \"black\") +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  ggthemes::theme_economist()+\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n5.1.2 Color, Size and Legends\n\n5.1.2.1 Color\nTo specify a single color, the most common way is to specify the name (e.g., \"red\") or the Hex code (e.g., \"#69b3a2\").\nYou can also specify an entire color palette. Some of the most common packages to work with color palettes in R are RColorBrewer and viridis. Viridis is designed to be color-blind friendly, and RColorBrewer has a web application where you can explore your data requirements and preview various palettes.\nFirst, if you want to run these examples install and load the RColorBrewer and viridis packages:\n\ninstall.packages(\"RColorBrewer\")\ninstall.packages(\"viridis\")\n\n\nlibrary(RColorBrewer)\nlibrary(viridis)\n\nNow, lets color our points using the palettes in viridis. To customize continuous color scales with viridis we use scale_color_viridis().\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income)) +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  viridis::scale_colour_viridis()\n\n\n\n\nSecond, let’s see how to do that with an RColorBrewer palette, using the ‘Greens’ palette and scale_color_distiller() function. We add direction = 1 to make it so that darker green is associated with higher values for income.\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income)) +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  scale_color_distiller(palette = \"Greens\", direction = 1)\n\n\n\n\n\n5.1.2.2 Size\nYou can edit the range of the point radius with scale_radius :\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income)) +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  scale_color_distiller(palette = \"Greens\", direction = 1)+\n  scale_radius(range = c(0.5, 6))\n\n\n\n\n\n5.1.2.3 Legends\nIn the previous plots we notice that two separate legends are created for size and color. To create one legend where the circles are colored, we use guides() like this, specifying the same title for color and size:\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income)) +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  scale_color_distiller(palette = \"BuGn\", direction = 1)+\n  scale_radius(range = c(2, 6))+\n  theme_minimal()+\n  #customize legend\n  guides(color= guide_legend(title = \"Median Income\"), size=guide_legend(title = \"Median Income\"))\n\n\n\n\n\n5.1.3 Annotation\nAnnotation is the process of adding text, or ‘notes’ to your charts. Say we wanted to highlight some details to specific points in our data, for example some of the outliers.\nWhen investigating the outlying point with the highest median age and high percentage of people of color, it turns out that census tract includes Rocky Mountain National Park and the surrounding area, and also the total population of that tract is only 53. Lets add these details to our chart with annotate(). This function requires several arguments:\n\ngeom: type of annotation, most often text\nx: position on the x axis to put the annotation\ny: position on the y axis to put the annotation\nlabel: what you want the annotation to say\nOptional: color, size, angle, and more.\n\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income)) +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\")+\n  scale_color_distiller(palette = \"BuGn\", direction = 1)+\n  scale_radius(range = c(2, 6))+\n  theme_minimal()+\n  guides(color= guide_legend(title = \"Median Income\"), size=guide_legend(title = \"Median Income\"))+\n  # add annotation\n  annotate(geom = \"text\", x=76, y = 62,\n           label = \"Rocky Mountain National Park region \\n Total Populaion: 53\")\n\n\n\n\nWe can also add an arrow to point at the data point the annotation is referring to with geom_curve and a few other arguments like so:\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income)) +\n  ggtitle(\"Census Tract socioeconomic data for Larimer County\") +\n  xlab(\"Median Age\") +\n  ylab(\"People of Color (%)\") +\n  scale_color_distiller(palette = \"BuGn\", direction = 1) +\n  scale_radius(range = c(2, 6)) +\n  theme_minimal() +\n  guides(color = guide_legend(title = \"Median Income\"),\n         size = guide_legend(title = \"Median Income\")) +\n  annotate(geom = \"text\",\n           x = 74,\n           y = 62,\n           label = \"Rocky Mountain National Park region \\n Total Populaion: 53\") +\n  # add arrow\n  geom_curve(\n    aes(\n      x = 82,\n      xend = 88,\n      y = 60,\n      yend = 57.5\n    ),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    size = 0.5,\n    curvature = -0.3\n  )\n\n\n\n\n\nNote that with annotations you may need to mess around with the x and y positions to get it just right. Also, the preview you see in the ‘plot’ window may look jumbled and viewing it by clicking ‘Zoom’ can help.\n\n\n5.1.4 Finalize and save\nWe are almost done with this figure. I am going to add/change a few more elements below. Feel free to add your own!\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income), alpha = 0.9) +\n  labs(\n    title = \"Socioeconomic data for Larimer County\",\n    subtitle = \"Median age, median income, and percentage of people of color for each census tract\",\n    x = \"Median Age\",\n    y = \"People of Color (%)\",\n    caption = \"Data obtained from the U.S. Census 5-year American Community Survey Samples for 2017-2021\"\n  )+\n  scale_radius(range = c(2, 6)) +\n  theme_classic() +\n  scale_color_viridis() + #use the Viridis palette\n  guides(color = guide_legend(title = \"Median Income\"),\n         size = guide_legend(title = \"Median Income\")) +\n  theme(\n    axis.title = element_text(face = \"bold\", size = 10),\n    plot.title = element_text(face = \"bold\",size = 15, margin = unit(c(1,1,1,1), \"cm\")),\n    plot.subtitle = element_text(size = 10, margin = unit(c(-0.5,0.5,0.5,0.5), \"cm\")),\n    plot.caption = element_text(face = \"italic\", hjust = -0.2),\n    plot.title.position = \"plot\", #sets the title to the left\n    legend.position = \"bottom\",\n    legend.text = element_text(size = 8)\n  ) +\n  annotate(geom = \"text\",\n           x = 74,\n           y = 62,\n           label = \"Rocky Mountain National Park region \\n Total Populaion: 53\",\n           size = 3,\n           color = \"black\") +\n  geom_curve(\n    aes(\n      x = 82,\n      xend = 88,\n      y = 60,\n      yend = 57.5\n    ),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    size = 0.5,\n    color = \"black\",\n    curvature = -0.3\n  )\n\n\n\n\nWant to make it dark theme?\nggdark is a fun package to easily convert your figures to various dark themes. If you want to test it out, install the package and try dark_theme_classic() instead of theme_classic() in the previous figure:\n\ninstall.packages(\"ggdark\")\n\n\nlibrary(ggdark)\n\n\ncensus_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income), alpha = 0.9) +\n  labs(\n    title = \"Socioeconomic data for Larimer County\",\n    subtitle = \"Median age, median income, and percentage of people of color for each census tract\",\n    x = \"Median Age\",\n    y = \"People of Color (%)\",\n    caption = \"Data obtained from the U.S. Census 5-year American Community Survey Samples for 2017-2021\"\n  )+\n  scale_radius(range = c(2, 6)) +\n  dark_theme_classic() +\n  scale_color_viridis() + #use the Viridis palette\n  guides(color = guide_legend(title = \"Median Income\"),\n         size = guide_legend(title = \"Median Income\")) +\n  theme(\n    axis.title = element_text(face = \"bold\", size = 10),\n    plot.title = element_text(face = \"bold\",size = 15, margin = unit(c(1,1,1,1), \"cm\")),\n    plot.subtitle = element_text(size = 10, margin = unit(c(-0.5,0.5,0.5,0.5), \"cm\")),\n    plot.caption = element_text(face = \"italic\", hjust = -0.2),\n    plot.title.position = \"plot\", #sets the title to the left\n    legend.position = \"bottom\",\n    legend.text = element_text(size = 8)\n  ) +\n  annotate(geom = \"text\",\n           x = 74,\n           y = 62,\n           label = \"Rocky Mountain National Park region \\n Total Populaion: 53\",\n           size = 3) +\n  geom_curve(\n    aes(\n      x = 82,\n      xend = 88,\n      y = 60,\n      yend = 57.5\n    ),\n    arrow = arrow(length = unit(0.2, \"cm\")),\n    size = 0.5,\n    curvature = -0.3\n  )\n\n\n\n\nSaving with ggsave\nYou can save your plot in the “Plots” pane by clicking “Export”, or you can also do it programmatically with ggsave(), which also lets you customize the output file a little more. Note that you can give the argument a variable name of a ggplot object, or by default it will save the last plot in the “Plots” pane.\n\n#specify the file path and name, and height/width (if necessary)\nggsave(filename = \"data/census_plot.png\", width = 6, height = 5, units = \"in\")\n\n\n5.1.4.1 Want to make it interactive?\nThe plotly package and the ggplotly() function lets you make your charts interactive.\n\ninstall.packages(\"plotly\")\n\n\nlibrary(plotly)\n\nWe can put our entire ggplot code above inside ggplotly() below to make it interactive:\n\nggplotly(census_data %>%\n  ggplot(aes(x = median_age, y = percent_bipoc)) +\n  geom_point(aes(size = median_income, color = median_income), alpha = 0.9) +\n  labs(\n    title = \"Socioeconomic data for Larimer County\",\n    subtitle = \"Median age, median income, and percentage of people of color for each census tract\",\n    x = \"Median Age\",\n    y = \"People of Color (%)\",\n    caption = \"Data obtained from the U.S. Census 5-year American Community Survey Samples for 2017-2021\"\n  )+\n  scale_radius(range = c(2, 6)) +\n  dark_theme_classic() +\n  scale_color_viridis() + #use the Viridis palette\n  guides(color = guide_legend(title = \"Median Income\"),\n         size = guide_legend(title = \"Median Income\")) +\n  theme(\n    axis.title = element_text(face = \"bold\", size = 10),\n    plot.title = element_text(face = \"bold\",size = 15, margin = unit(c(1,1,1,1), \"cm\")),\n    plot.subtitle = element_text(size = 10, margin = unit(c(-0.5,0.5,0.5,0.5), \"cm\")),\n    plot.caption = element_text(face = \"italic\", hjust = -0.2),\n    plot.title.position = \"plot\", #sets the title to the left\n    legend.position = \"bottom\",\n    legend.text = element_text(size = 8)\n  ))\n\n\n\n\n\nNote that we removed the annotations as plotly doesn’t yet support them."
  },
  {
    "objectID": "data-visualization-in-r.html#the-assignment",
    "href": "data-visualization-in-r.html#the-assignment",
    "title": "\n5  Data Visualization in R\n",
    "section": "\n5.2 The Assignment",
    "text": "5.2 The Assignment\nThis week’s assignment is to use anything you’ve learned today, in previous lessons and additional resources (if you want) to make two plots. One ‘good plot’ and one ‘bad plot’. Essentially you will first make a good plot, and then break all the rules of data viz and ruin it. For the bad plot you must specify two things that are wrong with it (e.g., it is not color-blind friendly, jumbled labels, wrong plot for the job, poor legend or axis descriptions, etc.) Be as ‘poorly’ creative as you want!\nYou can create these plots with any data (e.g., the census data from today, the penguins data past lessons, or new ones!), the good (and bad) visualization just has to be something we have not made in class before.\nTo submit the assignment, create an R Markdown document that includes reading in of the data, and the code to make the good figure and the bad figure. You will render your assignment to Word or HTML (and make sure both code and plots are shown in the output), and don’t forget to add the two reasons (minimum) your bad figure is ‘bad’. You will then submit this rendered document on Canvas. (20 pts. total)\nNote: the class will vote on the worst bad plot and the winners will receive extra credit! First place will receive 5 points, second place 3 points and third place 1 point of extra credit.\n\n\n5.2.1 Acknowledgements and Resources\nThe ggplot2 content in this lesson was created with the help of Advanced data visualization with R and ggplot2 by Yan Holtz. For more information on working with census data in R check out Analyzing US Census Data by Kyle Walker (which includes a visualization chapter)."
  },
  {
    "objectID": "data-analysis-t-test-and-anova.html#explore-the-dataset",
    "href": "data-analysis-t-test-and-anova.html#explore-the-dataset",
    "title": "\n6  Data Analysis: T-test and ANOVA\n",
    "section": "\n6.1 Explore the dataset",
    "text": "6.1 Explore the dataset\nWe will be using the and_vertebrates dataset for this lesson. Do a little exploration of this data first to understand its structure, variables and data types:\n\n# View the data structure\nglimpse(and_vertebrates)\n\n# Explore the metadata in the Help pane\n?and_vertebrates\n\nThis data set contains length and weight observations for three aquatic species in clear cut and old growth coniferous forest sections of Mack Creek in HJ Andrews Experimental Forest in Oregon. The three species are Cutthroat trout, Coastal giant salamander and Cascade torrent salamander."
  },
  {
    "objectID": "data-analysis-t-test-and-anova.html#t-test---compare-two-means",
    "href": "data-analysis-t-test-and-anova.html#t-test---compare-two-means",
    "title": "\n6  Data Analysis: T-test and ANOVA\n",
    "section": "\n6.2 t-test - Compare two means",
    "text": "6.2 t-test - Compare two means\nPrevious work has shown that forest harvesting can impact aquatic vertebrate biomass (Kaylor & Warren 2017). With this and_vertebrates dataset we can investigate this hypothesis, by comparing weight to forest type (clear cut or old growth). This therefore involves a test comparing the means (average weight) among two groups (clear cut and old growth forests), which then requires a t-test.\nLet’s focus on conducting this test for just Cutthroat trout to reduce species-level variances in weight. Before conducting analyses, we need to clean our dataset. The steps below will filter our data to just include the trout species, and remove any NA values of weight with the drop_na() function:\n\n#create a new variable for downstream analysis\ntrout_clean <- and_vertebrates %>% \n  # filter species (remember spelling and capitalization are IMPORTANT)\n  filter(species == \"Cutthroat trout\") %>% \n  # remove NA values for weight\n  drop_na(weight_g)\n\nBefore conducting any analyses, we may want to visualize the relationship between forest type and weight, which we can do with a boxplot given we have a categorical predicator variable (forest type, aka section) and a continuous response variable (weight_g).\n\ntrout_clean %>%\n  ggplot(aes(x = section, y = weight_g)) +   \n  geom_boxplot()\n\n\n\n\nWe don’t see too much of a difference based on this visual, but let’s conduct the statistical test to verify if our hypothesis is supported.\n\n6.2.1 Assumptions\nFirst however we need to check our test assumptions, which for t-tests assumes the variance of the groups is equal. We can test for equal variances with the function levene_test(), which performs a Levene’s test for homogeneity of variance across groups where the null hypothesis is that the variances are equal. In this function we specify the continuous, dependent variable (weight_g) and the predictor variable we want to test for variances between groups (section). We write this as a formula using ~ , which reads ’test if weight varies by forest section`.\n\ntrout_clean %>% \n  levene_test(weight_g ~ section)\n\n# A tibble: 1 × 4\n    df1   df2 statistic        p\n  <int> <int>     <dbl>    <dbl>\n1     1 12592      42.5 7.28e-11\n\n\nLooks like our variances are not equal, since the null hypothesis of the variance test is that they are equal and we have a (very) significant p-value. We have two options now, 1) we can transform our weight variable or 2) use the non-parametric Welch t-test which does not assume equal variances.\n\n6.2.1.1 Variable transformation\n\nIf we look at the distribution of weight (our continuous variable), it is pretty right skewed. Therefore, we’d likely want to do a natural log transformation on the data, which works well when the data is skewed like this:\n\nhist(trout_clean$weight_g)\n\n\n\n\nLet’s perform the variances check like we did before, but on the natural log transformed values, which you can do with log() , and we can nest the functions so we only use one line of code like this:\n\ntrout_clean %>% \n  levene_test(log(weight_g) ~ section)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     1 12592     0.468 0.494\n\n\nNow we have a high, insignificant p-value, indicating support for the null that the variances are equal. So. we can use the default t_test() function’s test which assumes equal variances, but on a natural-log transformed weight variable. For the t_test() function, it needs column names and we cannot nest a column name within a function like log(). Therefore we will need to use mutate() to create a new column for our log transformed weight variable.\nThe order of the variables in the t_test() function is {dependent variable} ~ {independent variable}. We use the ~ to specify a model/formula, similar to that of the levene_test(), telling the test we want to know if weight varies by forest section. We also set var.equal = TRUE to specify we know our groups have equal variances and detailed = TRUE to return the detailed results including group means (a.k.a., estimates).\n\ntrout_clean %>% \n  mutate(weight_log = log(weight_g)) %>% \n  t_test(weight_log ~ section, var.equal = TRUE, detailed = TRUE)\n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1 group2    n1    n2 statistic       p\n*    <dbl>     <dbl>     <dbl> <chr> <chr>  <chr>  <int> <int>     <dbl>   <dbl>\n1    0.131      1.52      1.39 weig… CC     OG      6798  5796      5.49 4.06e-8\n# ℹ 5 more variables: df <dbl>, conf.low <dbl>, conf.high <dbl>, method <chr>,\n#   alternative <chr>\n\n\nThe output of this test gives us the test statistics, p-value, and the means for each of our forest groups (estimate1 and estimate2, corresponding to group1 and group2). Given the extremely small p-value and the means of each group, we can conclude that Cutthroat trout weight was observed to be significantly higher in clear cut forests compared to old growth forests. But remember - these mean weight values are log transformed and not the raw weight in grams. The relationship can still be interpreted the same.\nHow does this relate to our original hypothesis?\nWelch Two Sample t-test\nAlternatively, instead of transforming our variable we can actually change the default t_test() argument by specifying var.equal = FALSE, which will then conduct a Welch t-test, which does not assume equal variances among groups.\n\ntrout_clean %>% \n  t_test(weight_g ~ section, var.equal = FALSE, detailed = TRUE) \n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.      group1 group2    n1    n2 statistic\n*    <dbl>     <dbl>     <dbl> <chr>    <chr>  <chr>  <int> <int>     <dbl>\n1     1.17      9.38      8.21 weight_g CC     OG      6798  5796      6.69\n# ℹ 6 more variables: p <dbl>, df <dbl>, conf.low <dbl>, conf.high <dbl>,\n#   method <chr>, alternative <chr>\n\n\nWhile we used a slightly different method, our conclusions are still the same, finding that Cutthroat trout have significantly higher weights in clear cut forests than old growth."
  },
  {
    "objectID": "data-analysis-t-test-and-anova.html#anova-test---compare-more-than-two-means",
    "href": "data-analysis-t-test-and-anova.html#anova-test---compare-more-than-two-means",
    "title": "\n6  Data Analysis: T-test and ANOVA\n",
    "section": "\n6.3 ANOVA test - compare more than two means",
    "text": "6.3 ANOVA test - compare more than two means\nWe found a significant difference in weight among forest types, but how about channel types? The unittype variable is categorical like section was, but here we have more than two categories. With more than two categories we use an ANOVA test instead of a t-test to assess significant differences in some continuous variable among groups.\nSince we found significant size differences between forest types, for the next analysis let’s just compare weight among channel types for one type of forest section to remove effects of forest type. Let’s look at clear-cut forests where trout were found to be significantly larger:\n\n# create a new variable to use for downstream analysis\ntrout_cc <- trout_clean %>% \n  filter(section == \"CC\")\n\nNow let’s first look at the distribution of trout samples among different channel types in our new filtered dataset:\n\ntrout_cc %>% \n  group_by(unittype) %>% \n  count()\n\n# A tibble: 6 × 2\n# Groups:   unittype [6]\n  unittype     n\n  <chr>    <int>\n1 C         3911\n2 P         1400\n3 R           97\n4 S            7\n5 SC        1000\n6 <NA>       383\n\n\nWe see there are quite a few samples with missing information for channel type. Also, there are some groups that have relatively low sample size. For the sake of this lesson, let’s just keep the three most abundant channel types: C (cascade), P (pool), and SC (side channel).\n\n# override our trout_cc variale with our groups of interest\ntrout_cc <- trout_cc %>% \n  # note this line of code is the same as doing filter(!is.na(unittype))\n    drop_na(unittype) %>% \n    filter(unittype %in% c(\"C\", \"P\", \"SC\"))\n\n\n6.3.1 Assumptions\nNormality\nANOVA assumes normal distributions within each group. Here our group sample sizes are >30 each which can be considered as large enough to not worry about this assumption. In fact the Shapiro-Wilk test won’t even operate if your group sizes are greater than 5,000. But, we can use the shapiro_test() function along with group_by() to test for normality within each channel group.\n\ntrout_cc %>% \n  group_by(unittype) %>% \n  shapiro_test(weight_g)\n\n# A tibble: 3 × 4\n  unittype variable statistic        p\n  <chr>    <chr>        <dbl>    <dbl>\n1 C        weight_g     0.794 3.48e-57\n2 P        weight_g     0.847 1.01e-34\n3 SC       weight_g     0.632 8.64e-42\n\n\nSince the null hypothesis of the Shapiro-Wilk test is that the data is normally distributed, these results tell us all groups do not fit a normal distribution for weight.\nEqual Variances\nTo test for equal variances among more than two groups, it is easiest to use a Levene’s Test like we did earlier:\n\ntrout_cc %>% \n  levene_test(weight_g ~ unittype)\n\n# A tibble: 1 × 4\n    df1   df2 statistic        p\n  <int> <int>     <dbl>    <dbl>\n1     2  6308      126. 3.19e-54\n\n\nGiven this small p-value, we see that the variances of our groups are not equal.\nTherefore, after checking out assumptions we need to perform a non-parametric ANOVA test, which is the Kruskal-Wallis test. We can do this with the kruskal_test() function specifying a formula like we have for previous tests to see if weight varies by channel (aka unittype).\n\ntrout_cc %>% \n  kruskal_test(weight_g ~ unittype)\n\n# A tibble: 1 × 6\n  .y.          n statistic    df         p method        \n* <chr>    <int>     <dbl> <int>     <dbl> <chr>         \n1 weight_g  6311      542.     2 2.05e-118 Kruskal-Wallis\n\n\nOur results here are highly significant, meaning that at least one of our group means is significantly different from the others.\nLet’s visualize the spread of weight among each group, looking at group summaries with a geom_boxplot() and group weight distributions with geom_histogram() and facet_wrap()\n\ntrout_cc %>% \n  ggplot(aes(x = unittype, y = weight_g, color = unittype)) + \n  geom_boxplot()\n\n\n\n\n\ntrout_cc %>% \n  # since we are making a histogram only need an 'x' variable\n  ggplot(aes(x = weight_g)) +\n  geom_histogram() +\n  # separate plot by unittype\n  facet_wrap(~unittype, ncol = 1)\n\n\n\n\n\n6.3.1.1 Post-Hoc Analysis\nNow ANOVAs don’t tell us which groups are significantly different, for that we would need to use a post-hoc test. Since we used the non-parametric Kruskal-Wallace test, we can use the associated Dunn’s test for multiple comparisons to assess significant differences among each pair of groups.\nIMPORTANT: if your data MEETS the ANOVA assumptions you would use the anova_test() function instead, and tukey_hsd() as the associated post-hoc test.\n\ntrout_cc %>% \n  dunn_test(weight_g ~ unittype)\n\n# A tibble: 3 × 9\n  .y.      group1 group2    n1    n2 statistic         p     p.adj p.adj.signif\n* <chr>    <chr>  <chr>  <int> <int>     <dbl>     <dbl>     <dbl> <chr>       \n1 weight_g C      P       3911  1400      15.2 5.05e- 52 1.01e- 51 ****        \n2 weight_g C      SC      3911  1000     -13.7 1.83e- 42 1.83e- 42 ****        \n3 weight_g P      SC      1400  1000     -23.1 4.18e-118 1.25e-117 ****        \n\n\nThis result shows us an adjusted p-value (for multiple comparisons) for each combination of groups, where a significant value represents a significant difference in weight between those two channel types. Our results here show that all channel types are significantly different in trout weight."
  },
  {
    "objectID": "data-analysis-t-test-and-anova.html#exercises",
    "href": "data-analysis-t-test-and-anova.html#exercises",
    "title": "\n6  Data Analysis: T-test and ANOVA\n",
    "section": "\n6.4 Exercises",
    "text": "6.4 Exercises\nEach question requires you to carry out a statistical analysis to test some hypothesis related to the and_vertebrates dataset. To answer each question fully:\n\nInclude the code you used to clean the data and conduct the appropriate statistical test. (Including the steps to assess and address your statistical test assumptions).\nReport the findings of your test.\nInclude an appropriate figure showing the patterns in the data associated with the test.\n\n\n1. Conduct a t-test similar to the one we carried out earlier in this lesson plan, but test for a difference in snout-vent length (length_1_mm) between forest types (section) for the Coastal giant salamander. (10 pts.)\n\n2. Conduct an ANOVA test to test for differences in snout-vent length between channel types (unittype, only using C, P, and SC channel types) for the Coastal Giant salamander. Remember to check your test assumptions and use the appropriate test based on your findings. You must also conduct the associated post-hoc test and report which groups are significantly different from each other, if any (10 pts.)\n\n\n6.4.1 Acknowledgements\nThanks to the developers of lterdatasampler for providing the data set and vignettes that helped guide the creation of this lesson plan.\n\n6.4.2 Citations\nData Source: Gregory, S.V. and I. Arismendi. 2020. Aquatic Vertebrate Population Study in Mack Creek, Andrews Experimental Forest, 1987 to present ver 14. Environmental Data Initiative. https://doi.org/10.6073/pasta/7c78d662e847cdbe33584add8f809165\nKaylor, M.J. and D.R. Warren. 2017. Linking riparian shade and the legacies of forest management to fish and vertebrate biomass in forested streams. Ecosphere 8(6). https://doi.org/10.1002/ecs2.1845"
  },
  {
    "objectID": "correlation-and-simple-linear-regression.html#correlation-with-salamanders",
    "href": "correlation-and-simple-linear-regression.html#correlation-with-salamanders",
    "title": "\n7  Correlation and Simple Linear Regression\n",
    "section": "\n7.1 Correlation with salamanders",
    "text": "7.1 Correlation with salamanders\nCorrelation measures the strength and direction of a relationship between two continuous variables. The key result of a correlation test, the correlation coefficient (r), ranges from -1 to +1, with 0 indicating no linear relationship, -1 a perfect negative relationship and 1 indicating a perfect positive relationship.\nWe will use the and_vertebrates data set to demonstrate the correlation test; we can test our length and weight continuous variables to see if “long” salamanders also weigh more.\nFirst, we’ll need to clean our data set to just include salamanders, and remove missing values for length and weight. Let’s focus on the variable length_2_mm for snout-to-tail length.\n\ndata(\"and_vertebrates\")\n\nsal <- and_vertebrates %>% \n  # find observations that contain the string \"salamander\" in the species column:\n  filter(str_detect(species, \"salamander\")) %>%\n  drop_na(length_2_mm, weight_g)\n\n\n7.1.0.1 EDA\nBefore diving in, let’s visually explore the relationship between our two variables with a scatter plot:\n\nggplot(sal) + \n  geom_point(aes(x = length_2_mm, y = weight_g), color = \"black\") +\n  theme_bw()\n\n\n\n\nFrom our plot, we see that there is indeed a pretty visible relationship between a salamander’s weight and length. In other words, a salamander’s weight and length seem to be positively correlated: as length increases, weight also increases, and vice versa.\n\n7.1.0.1.1 Parametric vs. non-parameteric correlation tests\nWhether we use a parametric or non-parametric correlation test depends on 1) whether our two variables exhibit a visually linear relationship and 2) whether they follow normal distributions. Looking at our plot above, we can see that their relationship does not appear linear. Let’s also check what the distributions of our variables look like:\n\nhist(sal$length_2_mm) # slightly skewed..\n\n\n\nhist(sal$weight_g) # def non normal distribution\n\n\n\n\nThey both look pretty skewed, therefore likely not normally distributed. We can statistically test if a variable fits a normal distribution with the shapiro_test() function. However, this function only runs for 5000 observations or less. Therefore, we will need to test for normality on a random subset of our full data set:\n\nsal_sub <- sal %>% slice_sample(n = 5000) \n  \nshapiro_test(sal_sub$length_2_mm)\n\n# A tibble: 1 × 3\n  variable            statistic  p.value\n  <chr>                   <dbl>    <dbl>\n1 sal_sub$length_2_mm     0.933 1.13e-42\n\nshapiro_test(sal_sub$weight_g)\n\n# A tibble: 1 × 3\n  variable         statistic  p.value\n  <chr>                <dbl>    <dbl>\n1 sal_sub$weight_g     0.559 1.87e-77\n\n\nThe null hypothesis of the Shapiro-Wilk normality test is that the variable is normally distributed, so a p-value less than 0.05, at 95% confidence (as we see for both of our variables here) tells use that our data does not fit a normal distribution.\nTherefore we have two options as we did with our t-test example in the previous lesson: transform the variables and see if that helps, or use a non-parametric test. Here, we will go ahead with the non-parametric Spearman correlation test.\n\nWhen two continuous variables are normally distributed and appear to have a linear relationship, one should use the Pearson correlation test.\n\n\n7.1.0.2 Correlation test in R\nIn R, we can perform this correlation test using the {rstatix}’s cor_test() function:\n\ncor_test(sal,\n         vars = c(length_2_mm, weight_g), # vector of continuous vars we want to test\n         alternative = \"two.sided\", # we want to test both positive and negative corr.\n         method = \"spearman\") # spearman is for non-parametric corr. method\n\n# A tibble: 1 × 6\n  var1        var2       cor  statistic     p method  \n  <chr>       <chr>    <dbl>      <dbl> <dbl> <chr>   \n1 length_2_mm weight_g  0.98 820565131.     0 Spearman\n\n\ncor is the correlation coefficient, r. In this example it is positive, indicating that there is a positive correlation between weight and length. Remember that r can only range from -1 to +1. So, a value of 0.98 indicates a very strong relationship.\nstatistic is the test statistic (t) which is calculated using r and the number of observations in the test.\np is the p-value. Here, it is calculated using the confidence level and the test statistic.\n\n7.1.0.3 Correlation limitations\n\nUsing a correlation test, we identified that a salamander’s weight and its length have a positive relationship. But, our conclusions have to stop there: all we can deduce from a correlation test is whether or not a relationship exists. There is no information about which one leads to the other (i.e., causality), and there is no information about how we might be able to predict one from the other."
  },
  {
    "objectID": "correlation-and-simple-linear-regression.html#simple-linear-regression-with-crabs",
    "href": "correlation-and-simple-linear-regression.html#simple-linear-regression-with-crabs",
    "title": "\n7  Correlation and Simple Linear Regression\n",
    "section": "\n7.2 Simple linear regression with crabs",
    "text": "7.2 Simple linear regression with crabs\nIn the summer of 2016, researchers surveyed fiddler crabs for their size across the Atlantic coast - from Florida to Massachusetts. Additional information about where the crabs lived (including coordinates, air, and water temperature) was also provided. Because we have size data across multiple climates, this study provides an excellent data set to test Bergmann’s rule.\nWith this data set, we can broadly assume that the further north we go, the colder the climate. Therefore, let’s use latitude as our variable to represent “coldness”, with an increase in latitude representing cooler climate conditions, to explore the relationship between crab size and climate.\nTaking correlation a step further, simple linear regression allows us to describe the relationship between two continuous, linearly-related variables. This description comes in the form of an equation that produces a line of best fit through our variables plotted against each other. Using that line, we can then predict a new crab’s size based on its location, and vice versa.\nBut first we must define which variable leads to the other’s change, something we didn’t really need to think about for correlation. Based on Bergmann’s rule, we know that the cooler climate leads to increased size, so our explanatory variable (x) will be latitude, and our response variable (y) will be crab size. There are a few ways to describe this relationship between latitude and size in the statistics lexicon:\n\n\nX Variable (Crab Latitude)\nY Variable (Crab Size)\n\n\nCause (what changes)\nEffect (outcome of that change)\n\n\nIndependent\nDependent\n\n\nPredictor\nOutcome\n\n\nExplanatory\nResponse\n\n\nLoad in the crab data from the lterdatasampler package:\n\ndata(\"pie_crab\")\n\nLet’s first visualize latitude vs. size with some histograms and a scatter plot.\n\nhist(pie_crab$latitude)\n\n\n\nhist(pie_crab$size)\n\n\n\n\nIt is clear that crab data was surveyed at only a finite number of locations, which leads to a non-normal distribution. But that’s okay - linear regression does NOT require the variables to be normally distributed. However, the error of the data must be normally distributed around the linear line (more on error later).\nWhen plotting, it is important to have the predictor variable on the x-axis, and the response variable on the y-axis. Then, we can fit a linear regression line through this data so that the distance between the line and the most observations is at its lowest:\n\nggplot(data = pie_crab, aes(x = latitude, y = size)) + \n  geom_point(color = \"black\") +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() \n\n\n\n\nEach observation’s vertical distance away from the line is the observation’s error. Getting the standard deviation of these distances gives us the line’s residual standard error.\nThe intercept of this line, or the value of the line if we extended it out to meet the y-axis (i.e., where our crab size is zero), is also known as the line’s constant.\nLastly, we have our slope of the line. This tells us how much a crab’s average size increases if we increase the latitude, and vice versa.\nWhen we put all of these pieces together, we get the following simple linear regression equation for each observation:\nsize = slope of the line * latitude + constant + point’s distance from line\n… which we write statistically as:\n\\(y = (β1 * x) + β0 + ε\\)\nIn this equation, y is our response variable (crab’s size), β0 is the constant, β1 is the slope, x is our explanatory variable (the crab’s latitude), and ε is the distance between the line and each observation’s true size.\n\n7.2.0.1 Simple linear regression in R\nIn R, we can identify the values that go into delineating this line of best fit using the base R {stats} package’s lm() function (lm for linear model):\n\n# Simple linear regression model\nslr_model <- lm(size ~ latitude, data = pie_crab) # lm(response ~ predictor, data = dataset)\n\nsummary(slr_model)\n\n\nCall:\nlm(formula = size ~ latitude, data = pie_crab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8376 -1.8797  0.1144  1.9484  6.9280 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.62442    1.27405  -2.845  0.00468 ** \nlatitude     0.48512    0.03359  14.441  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.832 on 390 degrees of freedom\nMultiple R-squared:  0.3484,    Adjusted R-squared:  0.3467 \nF-statistic: 208.5 on 1 and 390 DF,  p-value: < 2.2e-16\n\n\n… where -3.62442 is our line’s intercept (β0), 0.48512 is our line’s slope (β1):\n\\(y = 0.48512x - 3.62442\\)\nIn the model’s summary, our p-value is indicated in the Pr(>|t|) column for latitude: because our p-value is well below 0.05, we can deduce that latitude has a significant effect on crab size. Therefore, fiddler crabs appear to follow Bergmann’s rule: on average, crab size increased by 0.49 mm for every degree in latitude.\n\n7.2.0.2 Predicting crab size\nWith this linear equation, we can now predict crab size at different latitudes using the base R predict() function.\nLet’s predict crab size for latitudes of 32, 36, and 38 degrees. Note that we need to create these values as a new data frame with the same column name used in the data that the model was built off of (i.e., latitude):\n\nnew_lat <- tibble(latitude = c(32, 36, 38))\n\npredict(slr_model, newdata = new_lat)\n\n       1        2        3 \n11.89939 13.83987 14.81010"
  },
  {
    "objectID": "correlation-and-simple-linear-regression.html#exercises",
    "href": "correlation-and-simple-linear-regression.html#exercises",
    "title": "\n7  Correlation and Simple Linear Regression\n",
    "section": "\n7.3 Exercises",
    "text": "7.3 Exercises\nFor this week’s exercise, we will be using the ntl_airtemp and ntl_icecover data sets to explore the relationship between mean annual lake ice duration and mean winter air temperature at two nearby lakes in Wisconsin. We will answer the research question, “Is mean winter air temperature significantly related to mean annual lake ice duration, and can it be used to predict ice duration on lakes?”\nntl_airtemp contains daily estimates of the air temperature near the two lakes. ntl_icecover contains the duration of ice cover per year, per lake.\n\ndata(\"ntl_airtemp\")\ndata(\"ntl_icecover\")\n\nFirst, let’s get the average lake ice duration across years:\n\navg_icecover <- ntl_icecover %>%\n  # mutate within group by, and create a new variable for the WATER year (Oct - Sept). Water year is the FUTURE year so we do year + 1\n  group_by(wyear = year + 1) %>%\n  summarize(mean_duration = mean(ice_duration, na.rm = TRUE))\n\nNext, we will need to compute the mean winter (November-April) air temperature per water year to align with the data in avg_icecover. A water year is a 12-month period starting in October and ending in September, aligning with the winter season and thereby not splitting up the winter.\nLet’s first define each date’s water year using an if_else() statement and the month() function from the {lubridate} package:\n\nntl_airtemp_wyear <- ntl_airtemp %>%\n  mutate(wyear = if_else(month(sampledate) < 10, year, year+1))\n\nNext, using ntl_airtemp_wyear, we can compute the average air temperature for the winter season per water year.\n\nntl_winter_airtemp <- ntl_airtemp_wyear %>%\n  filter(lubridate::month(sampledate) %in% c(11, 12, 1:4)) %>% # filter the months from Nov to April\n  group_by(wyear) %>%\n  summarize(mean_air_temp = mean(ave_air_temp_adjusted))\n\n1. Join your table of (water-)yearly average winter temperatures to our avg_icecover object. Save this new table as icecover_temp. (HINT: use a join() function to do this.) (3 pts.)\n\n\n\n\n2. Visualize the data by plotting our variables against one another, and using histograms. Is their relationship linear? Are our variables normally distributed? (3 pts.)\n\n\n\n\n3. Perform a correlation test on icecover_temp to see whether there is a significant relationship between mean ice duration and mean air temperature. If so, is the relationship positive or negative? What is the correlation coefficient? (4 pts.)\n\n\n\n\n4. Develop a simple linear model to then predict the mean ice duration when mean winter temperatures are -2 degrees, 0 degrees, and 2 degrees. (4 pts.)\n\n\n\n\n5. Plot the mean ice cover duration against the mean air temperature. Include our simple linear regression (i.e., the line of best fit) in the plot. (3 pts.)\n\n\n\n\n6. What is the slope, intercept, and residual standard error of our simple linear regression? (3 pts.)"
  },
  {
    "objectID": "correlation-and-simple-linear-regression.html#citations",
    "href": "correlation-and-simple-linear-regression.html#citations",
    "title": "\n7  Correlation and Simple Linear Regression\n",
    "section": "\n7.4 Citations",
    "text": "7.4 Citations\nData Source: Anderson, L. and D. Robertson. 2020. Madison Wisconsin Daily Meteorological Data 1869 - current ver 32. Environmental Data Initiative. https://doi.org/10.6073/pasta/e3ff85971d817e9898bb8a83fb4c3a8b (Accessed 2021-03-08).\nData Source: Johnson, D. 2019. Fiddler crab body size in salt marshes from Florida to Massachusetts, USA at PIE and VCR LTER and NOAA NERR sites during summer 2016. ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/4c27d2e778d3325d3830a5142e3839bb (Accessed 2021-05-27).\nData Source: Magnuson, J.J., S.R. Carpenter, and E.H. Stanley. 2021. North Temperate Lakes LTER: Ice Duration - Madison Lakes Area 1853 - current ver 35. Environmental Data Initiative. https://doi.org/10.6073/pasta/ab31f2489ee436beb73fc8f1d0213d97 (Accessed 2021-03-08).\nMagnuson, J.J. 2021. Seeing the invisible present and place: from years to centuries with lake ice from Wisconsin to the Northern Hemisphere. Chapter 9 (243- 277) in R. B. Waide and S. E. Kingsland [eds]. The Challenges of Long Term Ecological Research: A Historical Analysis. Springer, Archimedes Series #59. https://doi.org/10.1007/978-3-030-66933-1_9 (Accessed 2022-02-14)."
  },
  {
    "objectID": "multiple-linear-regression-mlr.html#mlr-in-r",
    "href": "multiple-linear-regression-mlr.html#mlr-in-r",
    "title": "\n8  Multiple Linear Regression (MLR)\n",
    "section": "\n8.1 MLR in R",
    "text": "8.1 MLR in R\nRunning a multiple linear regression is very similar to the simple linear regression, but now we specify our multiple predictor variables by adding them together with a + sign (the order of our predictor variables does not matter). Here we are using the pie_crab data set again to develop a multiple linear regression model with additional variables from the data set:\n\ndata(pie_crab)\n\nmlr_model <- lm(size ~ latitude + air_temp + water_temp, data = pie_crab)\n\nsummary(mlr_model)\n\n\nCall:\nlm(formula = size ~ latitude + air_temp + water_temp, data = pie_crab)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7099 -1.7195 -0.0602  1.7823  7.7271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  77.7460    17.3477   4.482 9.76e-06 ***\nlatitude     -1.0587     0.3174  -3.336 0.000933 ***\nair_temp     -2.4041     0.3844  -6.255 1.05e-09 ***\nwater_temp    0.7563     0.1465   5.162 3.92e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.677 on 388 degrees of freedom\nMultiple R-squared:  0.4206,    Adjusted R-squared:  0.4161 \nF-statistic:  93.9 on 3 and 388 DF,  p-value: < 2.2e-16\n\n\n… where:\n77.7460 is our line’s intercept (β0)\n-1.0587 is the slope in the latitude dimension, or the estimated change in crab size for a unit change in latitude among crabs living with the same air temperature and water temperature conditions.\n-2.4041 is the slope in the air temperature dimension, or the estimated change in crab size for a unit change in air temperature among crabs living with the same water temperature and latitude conditions.\n0.7563 is the slope in the water temperature dimension, or the estimated change in crab size for a unit change in water temperature among crabs living with the same air temperature and latitude conditions.\n\\(y = -1.0587x1 -2.4041x2 + 0.7563x3 + 77.7460\\)\nIn the model’s summary, our p-value is indicated in the Pr(>|t|) column for each variable: because our p-values are well below 0.01, we can deduce that each variable has a significant effect on crab size.\nOur multiple R-squared (R2) is the Pearson correlation between the observed and the fitted (i.e. predicted) values. We can interpret this as 42.06% of the variability in crab size is explained by the linear regression on water temperature, air temperature, and latitude. NOTE: R2 always increases when an additional predictor is added to a linear model.\n\n8.1.1 Predicting crab size\nWith this multiple linear equation, we can now predict crab size across different varieties of latitude, air temperature, and water temperature using the base R predict() function:\n\nnew_data <- tibble(latitude = c(32, 36, 38),\n                   air_temp = c(20, 12, 9),\n                   water_temp = c(22, 14, 11))\n\npredict(mlr_model, newdata = new_data)\n\n       1        2        3 \n12.42241 21.37004 24.19604 \n\n\n\n8.1.2 MLR Assumptions\nAn important aspect when building a multiple linear regression model is to make sure that the following key assumptions are met:\nAll observations are independent of one another.\nThere must be a linear relationship between the dependent and the independent variables.\nAnd:\nThe variance of the residual errors is similar across the value of each independent variable.\n\nplot(mlr_model, which = 1)\n\n\nThis “Residuals vs Fitted” (fitted meaning the predicted values) plot gives an indication if there are non-linear patterns. This is a bit subjective, but a good way of verifying that this assumption is met is by ensuring that no clear trend seems so exist. The residuals should also occupy equal space above and below the line, and along the length of the line.\nThe residual error values are normally distributed.\n\nplot(mlr_model, which = 2)\n\n\n\n\n… also a bit subjective, but so long as the points on the Q-Q plot follow the dotted line, this assumption is fulfilled.\nThe independent variables are not highly correlated with each other.\nMulticolinearity can lead to unreliable coefficient estimates, while adding more variables to the model will always increase the R2 value, reflecting a higher proportion of variance explained by the model that is unjust.\n\npie_crab %>% \n  select(latitude, air_temp, water_temp) %>% \n  cor()\n\n             latitude   air_temp water_temp\nlatitude    1.0000000 -0.9949715 -0.9571738\nair_temp   -0.9949715  1.0000000  0.9632287\nwater_temp -0.9571738  0.9632287  1.0000000\n\n\nNormally, we should exclude variables that have a correlation coefficient greater than 0.7/-0.7. Alas, all of our variables are HIGHLY correlated with each other. Therefore, these predictors should not all be used in our model. Which is also to say… it is a good idea to check your predictor variables for colinearity before developing a model."
  },
  {
    "objectID": "multiple-linear-regression-mlr.html#exercises",
    "href": "multiple-linear-regression-mlr.html#exercises",
    "title": "\n8  Multiple Linear Regression (MLR)\n",
    "section": "\n8.2 Exercises",
    "text": "8.2 Exercises\nWe are interested in developing a multiple linear regression model to predict mean annual stream flow across the Eastern US. For every state, we have a handful of watershed and site characteristic data associated with USGS stream gauging stations.\nDownload the ‘usgs_gages’ folder on Canvas and store it in a ‘data’ folder in this assignment’s project directory. Here is a list of all of these files:\n\ndata_files <- list.files('data/usgs_gages', full.names = TRUE, pattern = \"*.csv\")\n\n1. Read in each of the data sets associated with the assignment and combine them into a single data set. (HINT: What does map_dfr() do)? 2.5 pts.\n\n\n\n\n2. Using our combined data set, plot mean annual stream flow against each variable to identify variables that seem to have a linear relationship with stream flow. 5 pts.\n\n\n\n\n3. Develop a multiple linear regression model using any combination of the variables in the data set. What is your R-squared value? Which of your variables (if any) are significant predictors of stream flow? 5 pts.\n\n\n\n\n4. Check to see if your model meets the model assumptions required for MLR. 2.5 pts.\n\n\n\n\n5. Use your model to predict mean annual stream flow for two new sets of predictor data. 2.5 pts.\n\n\n\n\n6. If your model does not meet the model’s assumptions, what are some ways of manipulating the data set so that it might? (HINT: review chapter 6) 2.5 pts."
  },
  {
    "objectID": "multiple-linear-regression-mlr.html#citations",
    "href": "multiple-linear-regression-mlr.html#citations",
    "title": "\n8  Multiple Linear Regression (MLR)\n",
    "section": "\n8.3 Citations",
    "text": "8.3 Citations\nData Source: Johnson, D. 2019. Fiddler crab body size in salt marshes from Florida to Massachusetts, USA at PIE and VCR LTER and NOAA NERR sites during summer 2016. ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/4c27d2e778d3325d3830a5142e3839bb (Accessed 2021-05-27).\nJohnson DS, Crowley C, Longmire K, Nelson J, Williams B, Wittyngham S. The fiddler crab, Minuca pugnax, follows Bergmann’s rule. Ecol Evol. 2019;00:1–9. https://doi.org/10.1002/ece3.5883"
  },
  {
    "objectID": "power.html#assignment",
    "href": "power.html#assignment",
    "title": "\n9  Power\n",
    "section": "\n9.1 Assignment",
    "text": "9.1 Assignment\nLet’s re-explore the difference in weight of cutthroat trout in clear cut (CC) and old growth (OG) forest types from our [T-test and ANOVA lesson][Data Analysis: T-test and ANOVA]. We want to see how sample size affects our ability to detect this difference. Therefore, our research question is: “Is there a significant difference in weight between old growth and clear cut forest types?” We will set our significance level at 0.05 (i.e., our test’s p-value must be below 0.05 for us to reject the null hypothesis).\nFirst load in your data (from the lterdatasampler package) and create a new variable trout for just the trout species:\n\ndata(and_vertebrates)\n\ntrout <- \n  and_vertebrates %>%\n  #filter species (remember spelling and capitalization are IMPORTANT)\n  filter(species == \"Cutthroat trout\") %>%\n  # remove NA values for weight, the variable we will be measuring\n  drop_na(weight_g) \n\nNext, we will select a random set of trout observations at both forest types across four different sample sizes: 5, 10, 1000, 5000.\nBelow is some template code for you to use to build your functions in Q1 and Q2, showing how to subset the data to 5 observations per forest type:\n\ntrout_5 <- trout %>% \n  #group by section to pull observations from each group\n  group_by(section) %>%\n  slice_sample(n = 5) %>%\n  # ungroup the data frame to perform the statistical test\n  ungroup() %>%\n  t_test(weight_g ~ section, var.equal = FALSE, detailed = TRUE)\n\n1. Write a function called trout_subber that returns a user-selected number of random observations (the thing that changes) from our trout data frame across both forest types (i.e., section) (5 pts.)\nHINTS: the number of observations you want to subset to will be an argument of the function. The code above can be used as the basis of the function. You can refer back to the Write Functions primer lessons for guidance, specifically How to write a function.\n\n\n\n\n2. Build upon the previous function by adding an additional step to perform a t-test on the data set at the end, so the function returns the results of the t-test. (NOTE: for simplicity, use the non-parametric t-test across all sub sets). (5 pts.)\n\n\n\n\n3. Map over the function above, using our sample sizes of interest (i.e., 5, 10, 1000, 5000 per forest type). Repeat the process 100 times for each sample size to account for variability. The final output of this exercise should be a single data frame with 400 rows (one row for each t-test summary). (5 pts.)\nHINTS: Make use of the rep() function to create your vector of numbers to map over…\n\n\n\n\n4. Using the data frame created in exercise 3, make a histogram of p-values for each sample size group (HINT: see what column name in your final data frame you should use to facet by). Make note of how the p-values and their variance change with sample size. (5 pts.)"
  },
  {
    "objectID": "power.html#citations",
    "href": "power.html#citations",
    "title": "\n9  Power\n",
    "section": "\n9.2 Citations",
    "text": "9.2 Citations\nData Source: Gregory, S.V. and I. Arismendi. 2020. Aquatic Vertebrate Population Study in Mack Creek, Andrews Experimental Forest, 1987 to present ver 14. Environmental Data Initiative. https://doi.org/10.6073/pasta/7c78d662e847cdbe33584add8f809165\nHorst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218."
  },
  {
    "objectID": "r-skills-review.html#tidying-datasets",
    "href": "r-skills-review.html#tidying-datasets",
    "title": "\n10  R Skills Review\n",
    "section": "\n10.1 Tidying datasets",
    "text": "10.1 Tidying datasets\nWe are interested in looking at how the Cache la Poudre River’s flow changes over time and space as it travels out of the mountainous Poudre Canyon and through Fort Collins.\nThere are four stream flow monitoring sites along the Poudre that we are interested in: two managed by the US Geological Survey (USGS), and two managed by the Colorado Division of Water Resources (CDWR):\n\n# Making a tibble to convert into coordinates for our sites\npoudre_sites <- tibble(site = c(\"Canyon Mouth\", \"Lincoln Bridge\", \"Boxelder\", \"Timnath\"),\n                       site_no = c(\"CLAFTCCO\", \"06752260\", \"06752280\", \"CLARIVCO\"),\n                       lat = c(40.6645, 40.5880833, 40.5519269, 40.5013),\n                       long = c(-105.2242, -105.0692222, -105.011365, -104.967),\n                       source = c(\"CDWR\", \"USGS\", \"USGS\", \"CDWR\")) %>%\n  sf::st_as_sf(coords = c(\"long\", \"lat\"), crs = 4269)\n\n# Mapview is another package that creates interactive plots, not necessary for you to know yet!\nmapview::mapview(poudre_sites, zcol = \"site\", layer.name = \"Poudre River Monitoring\")\n\n\n\n\n\n\n\nWe are going to work through retrieving the raw data from both the USGS and CDWR databases.\n\n10.1.1 Get USGS stream flow data\nUsing the dataRetrieval package we can pull all sorts of USGS water data. You can read more about the package, functions available, metadata etc. here: https://doi-usgs.github.io/dataRetrieval/index.html\n\n# pulls USGS daily ('dv') stream flow data for those two sites:\nusgs <- dataRetrieval::readNWISdv(siteNumbers = c(\"06752260\", \"06752280\"), # USGS site code for the Poudre River at the Lincoln Bridge and the ELC\n                                  parameterCd = \"00060\", # USGS code for stream flow\n                                  startDate = \"2020-10-01\", # YYYY-MM-DD formatting\n                                  endDate = \"2024-09-30\") %>% # YYYY-MM-DD formatting\n  rename(q_cfs = X_00060_00003) %>% # USGS code for stream flow units in cubic feet per second (CFS)\n  mutate(Date = lubridate::ymd(Date), # convert the Date column to \"Date\" formatting using the `lubridate` package\n         Site = case_when(site_no == \"06752260\" ~ \"Lincoln\", \n                          site_no == \"06752280\" ~ \"Boxelder\")) \n\n# if you want to save the data:\n#write_csv(usgs, 'data/review-usgs.csv')\n\n\n10.1.2 Get CDWR stream flow data\nAlas, CDWR doesn’t have an R packge to easily pull data from their API like USGS does, but they do have user-friendly instructions about how to develop API calls.\nDon’t stress if you have no clue what an API is! We will learn a lot more about them in later lessons, but this is good practice for our function writing and mapping skills.\nUsing the “URL Generator” steps outlined, if we wanted data from 2020-2024 for the Canyon mouth site (site abbreviation = CLAFTCCO), it generates this URL to retrieve that data:\nhttps://dwr.state.co.us/Rest/GET/api/v2/surfacewater/surfacewatertsday/?dateFormat=dateOnly&fields=abbrev%2CmeasDate%2Cvalue%2CmeasUnit&abbrev=CLAFTCCO&min-measDate=10%2F01%2F2020&max-measDate=09%2F30%2F2024\nHowever, we want to pull this data for two different sites, and may want to change the year range of data. Therefore, writing a custom function to pull data for our various sites and time frames would be useful:\n\n# Function to retrieve data\npull_cdwr <- function(site, start_year, end_year){\n  \n  raw_data <- httr::GET(url = paste0(\"https://dwr.state.co.us/Rest/GET/api/v2/surfacewater/surfacewatertsday/?dateFormat=dateOnly&fields=abbrev%2CmeasDate%2Cvalue%2CmeasUnit&abbrev=\",site,\n                                     \"&min-measDate=10%2F01%2F\", start_year,\n                                     \"&max-measDate=09%2F30%2F\", end_year))\n  \n  # extract the text data, returns a JSON object\n  extracted_data <- httr::content(raw_data, as = \"text\", encoding = \"UTF-8\") \n  \n  # parse text from JSON to data frame\n  final_data <- jsonlite::fromJSON(extracted_data)[[\"ResultList\"]]\n  \n  return(final_data)\n  \n}\n\nNow, lets use that function to pull data for our two CDWR sites of interest, which we can iterate over with map(). Since this function returns data frames with the same structure an variable names, we can use map_dfr() to bind the two data frames into a single one:\n\n# run function for our two sites\nsites <- c(\"CLAFTCCO\",\"CLARIVCO\")\n\ncdwr <- sites %>% \n  map_dfr(~ pull_cdwr(site = .x, start_year = 2020, end_year = 2024))\n \n# If you want to save this file\n#write_csv(cdwr, 'data/review-cdwr.csv') \n\n\n10.1.3 OR, read in the .csv’s we already generated and saved for you:\nRead in our two data sets. You will find that they provide the same information (daily streamflow from 2020-2024) but their variable names and structures are different:\n\nusgs <- read_csv('data/review-usgs.csv')\n\ncdwr <- read_csv('data/review-cdwr.csv') \n\nWhen we look at these two datasets, we see they provide the same information (daily streamflow from 2020-2024) but their variable names and structures are different:\n\nglimpse(usgs)\n\nRows: 2,920\nColumns: 6\n$ agency_cd        <chr> \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS\", \"USGS…\n$ site_no          <chr> \"06752260\", \"06752260\", \"06752260\", \"06752260\", \"0675…\n$ Date             <date> 2020-10-01, 2020-10-02, 2020-10-03, 2020-10-04, 2020…\n$ q_cfs            <dbl> 6.64, 7.41, 7.04, 6.84, 6.79, 7.81, 6.49, 11.30, 20.2…\n$ X_00060_00003_cd <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\"…\n$ Site             <chr> \"Lincoln\", \"Lincoln\", \"Lincoln\", \"Lincoln\", \"Lincoln\"…\n\nglimpse(cdwr)\n\nRows: 2,877\nColumns: 4\n$ abbrev   <chr> \"CLAFTCCO\", \"CLAFTCCO\", \"CLAFTCCO\", \"CLAFTCCO\", \"CLAFTCCO\", \"…\n$ measDate <date> 2020-10-01, 2020-10-02, 2020-10-03, 2020-10-04, 2020-10-05, …\n$ value    <dbl> 42, 36, 34, 32, 35, 30, 30, 34, 34, 27, 21, 29, 26, 43, 39, 4…\n$ measUnit <chr> \"cfs\", \"cfs\", \"cfs\", \"cfs\", \"cfs\", \"cfs\", \"cfs\", \"cfs\", \"cfs\"…\n\n\nTherefore, in order to combine these two datasets from different sources we need to do some data cleaning.\nLets first focus on cleaning the cdwr dataset to match the structure of the usgs one:\n\ncdwr_clean <- cdwr %>%\n  # rename data and streamflow vars to match name of usgs vars\n  rename(q_cfs = value) %>%\n  # Add site and agency vars\n  mutate(Date = lubridate::ymd(measDate),\n         Site = if_else(abbrev == \"CLAFTCCO\", \"Canyon\",\n                       \"Timnath\"),\n         agency_cd = \"CDWR\")\n\nNow, we can join our USGS and CDWR data frames together with bind_rows().\n\ndata <- bind_rows(usgs,cdwr_clean)"
  },
  {
    "objectID": "r-skills-review.html#exploratory-data-analysis",
    "href": "r-skills-review.html#exploratory-data-analysis",
    "title": "\n10  R Skills Review\n",
    "section": "\n10.2 Exploratory Data Analysis",
    "text": "10.2 Exploratory Data Analysis\nLet’s explore the data to see if there are any trends we can find visually. We can first visualize the data as time series:\n\n# Discharge (in CFS) through time displaying all four of our monitoring sites.\ndata %>%\n  ggplot(aes(x = Date, y = q_cfs, color = Site)) +\n  geom_line() +\n  theme_bw() +\n  xlab(\"Date\") +\n  ylab(\"Discharge (cfs)\") +\n  facet_wrap( ~ Site, ncol = 1)\n\n\n\n\nSay we wanted to examine differences in annual stream flow. We can do this with a little data wrangling, using the separate() function to split our “Date” column into Year, Month, and Day columns.\n\ndata_annual <- data %>% \n  separate(Date, into = c(\"Year\", \"Month\", \"Day\"), sep = \"-\") %>% \n  # create monthly avg for plots\n  group_by(Site, Year, Month) %>%\n  summarise(monthly_q = mean(q_cfs))\n\n# visualize annual differences across the course of each year\ndata_annual %>% \n  ggplot(aes(x = Month, y = monthly_q, group = Year)) +\n  geom_line(aes(colour = Year))+\n  facet_wrap(~Site) +\n  theme_bw()\n\n\n\n\nLet’s look at the daily difference in discharge between the mouth of the Cache la Poudre River (Canyon Mouth site) and each of the sites downstream. This will require some more wrangling of our data.\n\ndif_data <- data %>%\n  # select vars of interest\n  select(Site, Date, q_cfs) %>%\n  # pivot wider so each site is its own column\n  pivot_wider(names_from = Site, values_from = q_cfs) %>%\n  # for each downstream site, create a new column that is the difference from the Canyon mouth site\n  mutate_at(c(\"Boxelder\", \"Lincoln\", \"Timnath\"), .funs = list(dif = ~ (Canyon - .))) %>%\n  # then pivot these new columns (i.e., NOT the date and canyon columns) longer again\n  pivot_longer(-c(Canyon, Date)) %>% \n  # keep just the 'dif' values\n  filter(str_detect(name, \"_dif\"))\n\n\ndif_data %>% \n  # factor the site variable in order from distance to the canyon mouth for plotting purposes\n  mutate(name = fct(name, levels = c(\"Lincoln_dif\", \"Boxelder_dif\", \"Timnath_dif\"))) %>% \n  ggplot() +\n    geom_line(aes(x = Date, y = value, color = name)) +\n    theme_bw() +\n    facet_wrap(\"name\")+\n    ylab(\"Streamflow Difference from Poudre Mouth\")"
  },
  {
    "objectID": "r-skills-review.html#data-analysis",
    "href": "r-skills-review.html#data-analysis",
    "title": "\n10  R Skills Review\n",
    "section": "\n10.3 Data Analysis",
    "text": "10.3 Data Analysis\nThrough our exploratory data analysis, it appears that stream flow decreases as we move through town. But, how can we test if these flows are significantly different, and identify the magnitude/direction of these differences?\nBecause we will be comparing daily stream flow across multiple sites, we can use an ANOVA test to assess this research question. We will set our alpha at 0.05.\n\n10.3.1 Testing for normal distribution\nANOVA assumes normal distribution within each group - we can visualize each site’s data with a histogram:\n\nggplot(data = data, aes(x = q_cfs)) +\n         geom_histogram() + \n  facet_wrap (~Site)\n\n\n\n\n… and use the shapiro_test() function along with group_by() to statistically test for normality within each site’s daily stream flow data:\n\ndata %>%\n  group_by(Site) %>%\n  shapiro_test(q_cfs)\n\n# A tibble: 4 × 4\n  Site     variable statistic        p\n  <chr>    <chr>        <dbl>    <dbl>\n1 Boxelder q_cfs        0.474 3.29e-54\n2 Canyon   q_cfs        0.638 4.78e-48\n3 Lincoln  q_cfs        0.466 1.83e-54\n4 Timnath  q_cfs        0.403 8.79e-56\n\n\nSince the null hypothesis of the Shapiro-Wilk test is that the data is normally distributed, these results tell us all groups do not fit a normal distribution for daily stream flow. It is also quite clear from their histograms that they are not normally distributed.\n\n10.3.2 Testing for equal variance\nTo test for equal variances among more than two groups, it is easiest to use a Levene’s Test like we have done in the past:\n\ndata %>%\n  levene_test(q_cfs ~ Site)\n\n# A tibble: 1 × 4\n    df1   df2 statistic        p\n  <int> <int>     <dbl>    <dbl>\n1     3  5793      67.1 1.26e-42\n\n\nGiven this small p-value, we see that the variances of our groups are not equal.\n\n10.3.3 ANOVA - Kruskal-Wallis\nAfter checking our assumptions we need to perform a non-parametric ANOVA test, the Kruskal-Wallis test.\n\ndata %>%\n  kruskal_test(q_cfs ~ Site)\n\n# A tibble: 1 × 6\n  .y.       n statistic    df         p method        \n* <chr> <int>     <dbl> <int>     <dbl> <chr>         \n1 q_cfs  5797     1081.     3 4.20e-234 Kruskal-Wallis\n\n\nOur results here are highly significant (extremely small p-value), meaning that at least one of our sites has a stream flow significantly different from the others.\n\n10.3.4 ANOVA post-hoc analysis\nSince we used the non-parametric Kruskal-Wallace test, we can use the associated Dunn’s test to test across our sites for significant differences in mean stream flow:\n\ndata %>% \n  dunn_test(q_cfs ~ Site, detailed = TRUE)\n\n# A tibble: 6 × 13\n  .y.   group1   group2     n1    n2 estimate estimate1 estimate2 statistic\n* <chr> <chr>    <chr>   <int> <int>    <dbl> <dbl[1d]> <dbl[1d]>     <dbl>\n1 q_cfs Boxelder Canyon   1460  1460    1986.     2007.     3993.     32.1 \n2 q_cfs Boxelder Lincoln  1460  1460     615.     2007.     2622.      9.93\n3 q_cfs Boxelder Timnath  1460  1417     970.     2007.     2977.     15.5 \n4 q_cfs Canyon   Lincoln  1460  1460   -1371.     3993.     2622.    -22.1 \n5 q_cfs Canyon   Timnath  1460  1417   -1016.     3993.     2977.    -16.3 \n6 q_cfs Lincoln  Timnath  1460  1417     355.     2622.     2977.      5.69\n# ℹ 4 more variables: p <dbl>, method <chr>, p.adj <dbl>, p.adj.signif <chr>\n\n\nThe results of our Dunn test signify that all of our sites are significantly different from each other in terms of mean streamflow.\nTHOUGHT EXPERIMENT 1: Based on our results, which of our two gages have the greatest difference in mean daily stream flow?\nTHOUGHT EXPERIMENT 2: Is this an appropriate test to perform on stream flow data? Why or why not?"
  }
]